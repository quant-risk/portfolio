[
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "The blog posts on this website are all released under a Creative Commons Attribution 4.0 International License. Please feel free to use and share anything you find valuable in these posts, but please cite me too!"
  },
  {
    "objectID": "blog/sorte_apostas/index.html",
    "href": "blog/sorte_apostas/index.html",
    "title": "Sorte e Apostas",
    "section": "",
    "text": "Aqui voce pode encontrar a apresenta√ß√£o: Link da Apresenta√ß√£o\n\n\n\n\n\n\n\n\nYou can also reproduce this by choosing the city you want.\nCheck out the complete code on my Github.\n\n\n\n\n\n\nHey! üëã, did you find my work useful? Consider buying me a coffee ‚òï, by clicking here üëáüèª\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationFor attribution, please cite this work as:\nCosta, Henrique. 2024. ‚ÄúSorte e Apostas.‚Äù April 16, 2024.\nhttps://doi.org/10.59350/s9btn-bfx74."
  },
  {
    "objectID": "blog/regression/index.html",
    "href": "blog/regression/index.html",
    "title": "Trivia Series: Regression",
    "section": "",
    "text": "Rambling (proxy for an introduction)\nWe are experiencing the chaos of the information age, and its technological advents with voracious evolution and expansion. And in the midst of all this we have the advent of Big Data that has reached the delights of many with Data Science, Machine Learning and Artificial Intelligence.\nAs a result, the idea of mathematical and statistical ‚Äúmodels‚Äù became the rage, gaining visibility. And among these models we have the most trivial of all: regression.\nHave you ever stopped to think about where this term comes from, and what it means in practice? Exactly what you thought: ‚ÄúThat‚Äôs the basics!‚Äù That‚Äôs why this series of stories was called ‚Äútrivialities‚Äù, as they are trivialities that sometimes need to be known.\nOn my journey as a Quant backpacker (which is not that long, but long enough to make these observations) I realized that very simple points or concepts are not known to the overwhelming majority, and that is why I observed a lot of misapplication or erroneous judgment in statistical metrics , and I decided to start writing about it.\nAs I have academic roots, I brought the questions that will direct this text:\n\nWhat is regression?\nWhat does this term mean?\nWhere and when did it appear?\nHow to apply?\nAm I applying it correctly?\n\nAnd as usual, I‚Äôll start from the beginning (üòÑüòÑüòÑ).\n\n\nThe beginnings of the Regression\nWhen we think of this term, the idea of linear statistical regression immediately comes to mind. And yes, that‚Äôs right, the term is the same, but why regression? When I think about the meaning of the word, and obviously I already looked it up in a dictionary on the internet, which is the same as going back, regression comes from regressing, returning to a state that has already been overcome or that has evolved but returned to its previous levels. previous ones. It‚Äôs like climbing a ladder but going down backwards.\nAnd that‚Äôs right (üòÑ), regression is twisting, returning to the previous state. This term was coined in 1875 by an amateur mathematician named Francis Galton, who in some books says he was a first cousin of the famous Charles Darwin. Galton brought to the public the coining of this term, calling it ‚Äúregression to the mean‚Äù.\nWhy regression to the mean? Did something that moved end up returning to the mean? Yes that‚Äôs right. The concept only makes sense when we connect it with classical statistics, or better, with the central limit theorem.\nThis theorem is a right arm of probability theory, and builds the statement that when the sample size increases, the sampling distribution gets closer and closer to a normal distribution, and this is fundamental in statistical inference. Translating into simpler language: it‚Äôs like saying that when we observe nature, we perceive a natural state or a standard, normal state of things.\nAn example using Brazil, a country with a tropical climate, it is correct to say that the natural state of the climate is heat, as it is sunny many more days a year than cold or rainy, therefore, on average it is hotter, so the normal of Brazil is to have sunny days, but when it rains we can activate the idea of normality, in which the rain can soon stop and the sun and high temperatures return to the scene.\nRegression to the mean is exactly that, even if you have days with rain, the ‚Äúnormal‚Äù is that there are more sunny days, so rainy days are deviations between the averages, as the average is sunny days. But the term became known even after Galton published a study in 1885 in which he demonstrated, through regression calculations, that the height of children does not tend to reflect the height of their parents, but rather tends to regress towards the population average.\nIt is possible to see how the term gained scale after use by Galton, using the Google tool: Ngram book viewer.\n\n\n\nThe use of regression\nThe application of this method is known to many people, and can be found in different professional areas. But as I come from economic sciences, I will use this line to continue the text.\nThe economic sciences have econometrics as their quantitative tool, which is the metric for analyzing economic theories. In simpler language, it is the use of classical statistical techniques to analyze and test an economic theory. Economists use a lot of regression in econometrics.\nAnd studying econometrics I learned a lot about the fundamentals of regression analysis. But a few paragraphs above I wrote that regression is returning to your previous state, so how would economists use a matrix like that? For the simple fact that the economy works with the assumption of a state of general equilibrium of things.\nThe economist has a single objective, and amazingly, it is not to save money, it is to allocate scarce resources to meet unlimited needs. But how does he do this? Artificial intelligence!!! (üòÑüòÑüòÑ).\nJoking apart. The economist uses optimization as his beacon. Allocating scarce resources to satisfy unlimited needs is only possible by optimizing. And what does optimizing have to do with regression? All! Economists‚Äô assumption of a general equilibrium is only possible using optimization, minimizing costs and maximizing profits, and regressing to the mean or returning to a normal state only reinforces or supports the idea of general equilibrium.\nSo, I will bring a brief example of application, coming from a study material for economists, the book on basic econometrics by Damodar N. Gujarati and Dawn C. Porter, in which we find an application that is well known among economists: the hypothesis of marginal propensity to consume, or MPC.\nKeynes stated: The fundamental psychological law [‚Ä¶] is that men [women] are disposed, as a rule and on an average, to increase their consumption as their income increases, but not in the same proportion as the increase in income. (Keynes, John Maynard. The general theory of employment, interest and money. New York: Harcourt Brace Jovanovich, 1936. p.¬†96.).\nSo to simplify, we will use an econometric model based on a simple linear regression, but first I need to comment that there is a rationale behind a linear regression, which is the central objective of this text, to talk about some basic points that sometimes fall into oblivion .\nAn econometric model (statistical/mathematical) aims to represent the reality we are interested in investigating. The model must be able to capture the relationships between reality and theory, so that the theory can be tested. However, this representation of reality is not complete, a model alone is not capable of representing or describing reality as a whole, so it needs to be conditioned and sometimes restricted.\nKeynes‚Äô hypothesis brings exactly this idea, with the construction of a consumption function, and without many details, his hypothesis of a relationship between consumption and income seems deterministic or an exact relationship. So, as the model cannot have all the information, it uses the most likely or available information and relies on the premise that the other effects are constant or unchanged, not causing direct influence: ceteris paribus.\nI wrote a text about how I see the ceteris paribus condition, and how it helps us understand real-life phenomena. So, returning to the example of Keynes‚Äôs hypothesis, he uses ceteris paribus in an intuitive and implicit way, leaving no details, but establishing the premises: ‚Äúto increase your consumption as your income increases‚Äù. So we already have the relationship we need to build our model.\nIn short, Keynes postulated that the population had a tendency to increase its consumption when its income increases, which was labeled marginal propensity to consume (PMC), and it would be analyzed quantitatively as a rate of variation in consumption, and that this variation would be given in units (say, one dollar) of income, and that it will always be greater than zero, but less than 1, as his observations showed that additional consumption did not have the same level as additional income, that is, people did not they consumed everything they earned and that is why it has to be less than 1.\n\n\nSpecifying the hypothesis-based econometric model\nAlthough Keynes established an apparently positive relationship between income and consumption, he did not stipulate how this relationship happens. To simplify, let‚Äôs use ‚Äúpoetic license‚Äù and suggest the following functional form for the relationship between income and consumption established by Keynes:\n\\[ Y = \\alpha + \\beta X \\;\\;\\;\\;\\;\\;\\;\\; 0 &lt; \\beta &lt; 1 \\]\nwhere \\(Y\\) represents consumption expenditure and \\(X\\) represents income, and \\(\\alpha\\) and \\(\\beta\\) are the model parameters. These parameters represent the effects generated through the relationship between \\(X\\) and \\(Y\\). Sometimes known as marginal effects.\nThe parameter \\(\\alpha\\) is known as ‚Äúintecept‚Äù, or average value. Generally it demonstrates that where the relationship between the variables between \\(X\\) and \\(Y\\) begins. It is the average value, if the marginal effect is null (zero). It‚Äôs like saying that, if \\(X\\) has no influence on \\(Y\\), then when \\(X\\) and \\(Y\\) are related, it is \\(\\alpha\\) that shows the level of this relationship.\nThe parameter \\(\\beta\\) is known as ‚Äúangle‚Äù or ‚Äúangular coefficient‚Äù, also called the slope of a straight line, it determines the slope of a straight line. A straight line because the relationship between \\(X\\) and \\(Y\\) is direct (linear). The angular coefficient is a number that is related to the angle formed between the straight line and the horizontal, describing the slope of the straight line. And when we connect the terms ‚Äúslope‚Äù and ‚Äúangle‚Äù, we can remember the concept of derivation, and that is exactly it, it is the effect of the derivation in relation to \\(X\\). And we know that in economics the concept of marginality is directly correlated to the concept of derivation, which is why the angular coefficient is known as marginal effect.\nAnd the angular coefficient (\\(\\beta\\)) will be our indicator of the relationship between income and consumption, which we will call Marginal Propensity to Consume. This is already constructed by Keynes‚Äô theory, but we are here trying to work on an idea.\nAs I mentioned in the paragraphs above, this relationship does not represent a pure and exact reality. Of course, there are many variables that can affect consumption in addition to income, and therefore the model needs an additional specification, which for me is the charm of the linear regression model.To take into account the influences of other variables that were not imposed in this model:\n\\[ Y = \\alpha + \\beta X + u \\]\nwhere \\(u\\) is known as the disturbance, or error term of the model. It is a random (stochastic) variable that has probabilistic properties. The error term \\(u\\) is intended to represent all other factors that affect consumption but are not explicitly imposed here.\nThe error term is essentially the ceteris paribus condition. It contains all the other effects that interact with Consumption, but they are being nullified, or kept constant/unchanged, that is, there is no variation, so no effect can be felt.\nThat is, by isolating the effect of income on consumption, we artificially create a kind of general equilibrium, showing that the relationship between \\(X\\) and \\(Y\\) is balanced (normal), and that other factors have no influence.\n\n\nEstimating the model\nWith data from Table I.1 of the Gujarati basic econometrics book, which refers to the United States economy in the period 1960-2005. Table I.1 uses aggregate consumption to represent model consumption, and GDP to represent income, and thus we will estimate the marginal propensity to consume. With the data in hand, it‚Äôs time to go to RStudio and start ‚Äúthe work‚Äù. Let‚Äôs Code!\n\n\nShow the code\n# install.packages(\"janitor\")\n# remotes::install_github(\"brunoruas2/gujarati\")\n\n# load libraries\nlibrary(tidyverse)\nlibrary(gujarati)\n# library(janitor)\n\nTableI_1 %&gt;% \n  mutate(\n    Year = str_replace_all(Year, \" \", \"\"),\n    PCE.Y. = str_replace_all(PCE.Y., \" \", \"\"),\n    GDP.X. = str_replace_all(GDP.X., \" \", \"\"),\n  ) %&gt;% \n  as_tibble(.name_repair = janitor::make_clean_names) %&gt;% \n  mutate(\n    pce_y = as.numeric(pce_y),\n    gdp_x = as.numeric(gdp_x)\n  ) -&gt; tbl_pmc\n\n\n\nmodel &lt;- lm(pce_y ~ gdp_x, data = tbl_pmc)\n\n\nmodel %&gt;% \n  summary() %&gt;% \n  pander::pander()\n\n\n\n\n\n\n\n\n\n\n\n\n¬†\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n-299.6\n28.76\n-10.42\n1.877e-13\n\n\ngdp_x\n0.7218\n0.004423\n163.2\n7.244e-63\n\n\n\n\nFitting linear model: pce_y ~ gdp_x\n\n\n\n\n\n\n\n\nObservations\nResidual Std. Error\n\\(R^2\\)\nAdjusted \\(R^2\\)\n\n\n\n\n46\n73.57\n0.9984\n0.9983\n\n\n\n\n\nNow that we have an output from the experimental model, we can obtain numerical estimates of the parameters that will provide us with an empirical resolution for the consumption function proposed here in this example.\nNow that we have an output from the experimental model, we can obtain numerical estimates of the parameters that will provide us with an empirical resolution for the consumption function proposed here in this example.\nWe can note that the statistical technique of regression analysis is the main tool for obtaining such estimates. After running the model, we obtain the following estimates:\n\n\\(\\hat{\\alpha}\\) = -299.6\n\\(\\hat{\\beta}\\) = 0.7218\n\nNow we can empirically translate the estimated consumption function as:\n\\[ \\hat{Y_t} = \\hat{\\alpha} + \\hat{\\beta} X_t \\]\n\\[ \\hat{Y_t} = -299,5913 + 0,7218 X_t \\]\nThe caret above the Y and the parameters \\(\\alpha\\) and \\(\\beta\\) indicates that this is an estimate. The following graph shows this estimated consumption function (i.e., the regression line).\n\n\nShow the code\n# load libraries\nlibrary(tidyverse)\n\n\n\n# building a plot with ggplot2 based on grammar of graphics:\n\ntbl_pmc %&gt;% \nggplot(aes(\n  x = gdp_x, \n  y = pce_y\n  )) +\n  geom_point(aes(\n    color = \"red\", \n    size = .7, \n    alpha = .8\n    )) +\n  geom_smooth(\n    method = \"lm\", \n    se = TRUE,  \n    inherit.aes = TRUE\n    ) +\n  theme(legend.position = \"none\") +\n  labs(\n    title = \"Econometric model of the Keynesian consumption function\",\n    subtitle = \"Values for 1960-2005 in billions of 2000 dollars\",\n    x = \"Gross Domestic Product\",\n    y = \"Personal Consumption Expenditures\",\n    caption = \"Source: Economics Report of the President, 2007, Table B-2, p, 230.\")\n\n\n\n\n\n\n\n\n\nThe red dots represent our income data (GDP), and the blue line represents the regression line.\nAs we can see in the model graph, it is possible to say that the regression line (blue) fits the data well, this means that the points (red) on the graph that represent the data are very close to the regression line.\nThe table of model results and the graph show us that, in this analyzed period (1960-2005), a marginal effect of income (GDP) on consumption behavior of almost 0.72 is estimated.\nAs I already mentioned, this marginal effect estimate comes from the angular coefficient, which we will now call ‚ÄúCoefficient of the Marginal Propensity to Consume‚Äù, and this coefficient is telling us that, in the sampled period, whenever there is an increase of one dollar in income In real terms, on average, there is an additional increase of around 72 cents in real consumption expenditure, that is: for every 1 dollar more, there is, on average, a marginal effect of 72 cents. If it increases by 1 dollar, consumption increases by 72 cents, and the opposite is also true.\n\n\n\n\n\n\nAverage Marginal Effects\n\n\n\n\n\nAttention to a critical point: we use the term ‚Äúaverage‚Äù or we always use the phrase ‚Äúon average‚Äù because the relationship between consumption and income is inexact, that is, it is not a direct causal relationship; and this is very clear in the model graph, as not all red points in the data are exactly on the regression line.\nIn general terms, we can say that, ‚Äú[‚Ä¶]according to our data, average consumer spending increases by about 70 cents for every dollar increase in income.\nAnd this reinforces the idea of normality and average. We use the average as the optimal moment, because under normal conditions of temperature and pressure the effect is recognized in the average, because no matter the changes or oscillations, if we believe in the normality that after these variations everything returns to normal, then the average is the central point, that is, the normal.\nAs I tried to explain before, what we call marginal effects are partial derivatives of the regression equation in relation to each variable in the model; So therefore, the average marginal effects are simply the average of these partial derivatives. In ordinary least squares regression, the estimated slope coefficients (angular and/or \\(\\hat \\beta\\)) are marginal effects.\n\n\n\nNow we know that on the surface, Keynes‚Äô assumption appears to make sense. We found this positive relationship between income and consumption. However, we still cannot simply accept this value as a confirmation of the Keynesian consumption hypothesis, as it is not enough to just find the effect, it is necessary to test it, and that is science. And following the scientific standard, let‚Äôs create a problem question: ‚ÄúIs this estimate sufficiently below unity?‚Äù This is to convince us that it is not a result due to chance or a peculiar insight that the data we use is showing.\n\n\nTesting a hypothesis\nBased on the idea of a scientific test, in which the statistical framework is used as a tool, is the marginal effect of 0.72 statistically smaller than 1? If this is true, it will be support for the birth of a theory. This practice of testing, or better said, corroborating or refuting existing economic theories or those that are emerging, based on sample evidence, which is basically what we have just done, has a foundation based on statistical theory, in a field of study known as statistical inference (hypothesis testing).\nLet‚Äôs consider that the adjusted model we have just put together is a reasonably good approximation of reality, and that is one of the objectives of a model, but we need to be critical and criticize our own model, trying to understand if the estimates obtained are in accordance with the expectations of the theory being tested, which in this case is Keynes‚Äô theory of marginal propensity to consume.\nEconomist Milton Friedman said that a theory or hypothesis that is not verifiable with empirical evidence may not be admissible as part of scientific research. Whereas Keynes expected the marginal propensity to consume to be positive but less than 1. In our model, we estimate a marginal propensity of about 0.72. But before this figure can be accepted as a confirmation of Keynesian consumption theory, we need to statistically test this estimate to convince ourselves that it is not a result due to chance or a peculiarity of the data we use.\nPara isso, iremos utilizar do conceito de teste de hipotese. Nesse teste teremos que lidar com dois tipos de erros poss√≠veis de ocorrer:\n\nA hip√≥tese nula √© rejeitada embora seja verdadeira (erro tipo I)\nA hip√≥tese nula n√£o √© rejeitada embora seja falsa (erro tipo II)\n\n\n\n\n\n\n\nTip with Title\n\n\n\nThe significance level of the test is the probability to commit a type-I-error we are willing to accept in advance. E.g., using a prespecified significance level of 0.05, we reject the null hypothesis if and only if the p-value is less than 0.05. The significance level is chosen before the test is conducted.\nAn equivalent procedure is to reject the null hypothesis if the observed test statistic is, in absolute value terms, larger than the critical value of the test statistic. The critical value is determined by the significance level chosen and defines two disjoint sets of values which are called acceptance region and rejection region. The acceptance region contains all values of the test statistic for which the test does not reject while the rejection region contains all the values for which the test does reject.\nThe p-value is the probability that, in repeated sampling under the same conditions a test statistic is observed that provides just as much evidence against the null hypothesis as the test statistic actually observed.\nThe actual probability that the test rejects the true null hypothesis is called the size of the test. In an ideal setting, the size does equal the significance level.\nThe probability that the test correctly rejects a false null hypothesis is called power.\nHanck, Christoph, Martin Arnold, Alexander Gerber, and Martin Schmelzer. Introduction to Econometrics with R. Universit√§t Duisburg-Essen, 2021.\n\n\nYou can also reproduce this by choosing the city you want.\nCheck out the complete code on my Github.\n\n\n\n\n\n\nHey! üëã, did you find my work useful? Consider buying me a coffee ‚òï, by clicking here üëáüèª\n\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationFor attribution, please cite this work as:\nCosta, Henrique. 2023. ‚ÄúTrivia Series: Regression.‚Äù\nDecember 20, 2023. https://doi.org/10.59350/xs70x-mar87."
  },
  {
    "objectID": "blog/desafio_EconViz_01/index.html",
    "href": "blog/desafio_EconViz_01/index.html",
    "title": "Reproduzindo gr√°fico do Economista Visual - Valor da cesta b√°sica (PT-BR)",
    "section": "",
    "text": "Na publica√ß√£o de hoje, quero trazer uma brincadeira que fiz comigo mesmo, um pequeno incentivo para voltar a publicar conte√∫dos sobre R (eu j√° estava enferrujado) entre outros assunto sobre economia.\nPasseando pelo instagram, encontrei um perfil que p√∫blica muita informa√ß√£o sobre economia em formato de visualiza√ß√£o, o Economista Visual.\nVendo um gr√°fico sobre o pre√ßo da cesta b√°sica na cidade de S√£o Paulo - SP\n\nResolvi criar o desafio de reproduzir esta visualiza√ß√£o. Claro que n√£o conseguiria deixar igual, mas poderia chegar bem perto.\nVamos come√ßar pela fonte dos dados. Os dados utilizado no gr√°fico vem da base do DIEESE (Departamento Intersindical de Estat√≠stica e Estudos Socioecon√¥micos), que faz o levantamento do valor da Cesta B√°sica de Alimentos em v√°rias cidades do pa√≠s.\nAcessando o website do DIEESE, na pagina do banco de dados de Cesta B√°sica de Alimentos √© poss√≠vel escolher a cidade desejada e fazer o download da planilha contendo as informa√ß√µes sobre o valor da cesta b√°sica.\n\nCom os dados em m√£os, √© hora de ir para o RStudio e come√ßar ‚Äúos trabalhos‚Äù. Let‚Äôs Code!\n# para retirar notacoes cientifica\noptions(scipen = 999)\n\n\n# Pacotes necessarios para o projeto\n\nlibrary(tidyverse)     # conjunto de pacotes essenciais\nlibrary(readxl)        # pacote para \"ler\" arquivos excel\nlibrary(lubridate)     # pacote para trabalhar com datas\nlibrary(scales)        # pacote para trabalhar com escalas\nlibrary(here)          # fluxos de trabalho orientados a projetos\nParticularmente, prefiro utilizar um fluxo orientado a projetos e por isso o pacote hereest√° na lista acima. O pacote here implementa uma maneira mais simples de encontrar seus arquivos. O objetivo do pacote here √© permitir a refer√™ncia f√°cil de arquivos em fluxos de trabalho orientados a projetos. Em contraste com o uso setwd(), que √© fr√°gil e dependente da maneira como voc√™ organiza seus arquivos, here usa o diret√≥rio de n√≠vel superior de um projeto para construir facilmente caminhos para os arquivos.\nAp√≥s carregar os pacotes, √© hora de ler os dados que est√£o na planilha em formato .xls.\n# Para carregar os dados usaremos o codigo a seguir:\n\ncesta_basica &lt;- read_excel(     # funcao que ler arquivos .xls\n  here::here(                   # funcao que informa o local\n    \"dados\",\"exporta.xls\"       # \"local\", \"arquivo\"\n    )\n)\nAgora que os dados j√° est√£o no global environment, vamos usar de alguns ‚Äúcostumes‚Äù de pr√©-analise para conhecer os dados. Lembrando que qualquer modifica√ß√£o nos dados ser√° feito inteiramente e diretamente no R.\n# visualizar a situacao dos dados\nhead(cesta_basica)              # funcao imprime no console \n                                # as 10 primeiras observacoes\n\n## A tibble: 6 x 2\n#  `Gasto Mensal - Total da Cesta` ...2     \n#  &lt;chr&gt;                           &lt;chr&gt;    \n# 1 NA                              S√£o Paulo\n# 2 01-2000                         112.22   \n# 3 02-2000                         110.8    \n# 4 03-2000                         115.13   \n# 5 04-2000                         115.92   \n# 6 05-2000                         111.78 \nO resultado mostra o √≥bvio, que os dados precisam de ‚Äútratamento‚Äù antes de pensarmos em extrair qualquer informa√ß√£o. Aqui neste caso, precisa retirar a primeira linha, que cont√©m NA (not available) e uma palavra (S√£o Paulo) onde deveria ser n√∫meros. Os nomes das colunas (vari√°veis) tamb√©m necessitam de aten√ß√£o. Vamos olhar um pouco mais, com a fun√ß√£o tail().\ntail(cesta_basica)              # funcao imprime no console \n                                # as 10 ultimas observacoes\n\n# A tibble: 6 x 2\n#  `Gasto Mensal - Total da Cesta`    ...2          \n#   &lt;chr&gt;                              &lt;chr&gt;         \n# 1 11-2020                            629.179999999~\n# 2 12-2020                            631.460000000~\n# 3 01-2021                            654.149999999~\n# 4 Fonte: DIEESE                      NA            \n# 5 (1) S√©rie recalculada, conforme mudan√ßa metodol√≥gica realizada # na ~ NA            \n# 6 Tomada especial de pre√ßos a partir de abril de 2020 NA    \nComo esperado, s√≥ refor√ßa a necessidade de um tratamento simples nos dados. A priori √© uma ‚Äúlimpeza‚Äù de linhas indesejadas.\nPor boas pr√°ticas, irei modificar o ‚Äúnome‚Äù do objeto (data frame), pois CB √© mais simples e f√°cil que cesta_basica.\nCB &lt;- cesta_basica %&gt;%          # criando um objeto reduzindo\n                                # o nome do dataframe para CB\n                                # ao inves de cesta_basica\n                                # e usando o operador \" %&gt;% \"\n  \n  rename(                       # funcao para renomear colunas\n    Data = `Gasto Mensal - Total da Cesta`,\n    Valor = ...2\n  ) %&gt;%                       \n  slice(2:255)                  # funcao que corta os dados\n                                # retirando apenas o desejado\nAgora temos que verificar as caracter√≠sticas dos nossos dados. Utilizo a fun√ß√£o srt().\nstr(CB)\n\n# tibble [254 x 2] (S3: tbl_df/tbl/data.frame)\n# $ Data : chr [1:254] \"01-2000\" \"02-2000\" \"03-2000\" \"04-2000\" ...\n# $ Valor: chr [1:254] \"112.22\" \"110.8\" \"115.13\" \"115.92\" ...\nTemos 254 observacoes e 2 colunas (variaveis) as 2 variaveis estao em formato -chr- caracteres e precisamos modificar esses formatos, passando a variavel Valor para o formato n√∫merico, e a vari√°vel Data como datas, para isso utilizaremos os passos a seguir:\nCB &lt;- CB %&gt;%                   # substituindo o dataframe\n  mutate(                      # funcao p/ modificar variaveis\n    Data = rep(seq(            # Substituindo a variavel Data\n                               # por uma variavel no formato de datas\n                               # criando uma sequencia de datas\n      from = as.Date(\"2000-01-01\"),\n      to = as.Date(\"2021-01-01\"),\n      by =\"1 month\"\n    )),\n    Valor = as.numeric(Valor) # modificando a variavel ¬¥Valor¬¥\n                              # para o formato numerico\n  )\n\n# Erro: Problem with `mutate()` input `Data`.\n# x Input `Data` can't be recycled to size 254.\n# i Input `Data` is `rep(...)`.\n# i Input `Data` must be size 254 or 1, not 253.\nPercebemos esse erro que significa que o vetor de datas que tentamos criar nao √© do mesmo ‚Äútamanho‚Äù que a variavel Data anterior. A variavel Data tem 254 observacoes e nosso vetor de datas tem apenas 253. Totalmente sem sentido, pois criamos um vetor de datas que segue uma sequencia mes a mes de 01/01/2000 ate 01/01/2021.\nA priori, temos a hip√≥tese de que h√° valores e data repetidas como a base de dados eh pequena (254 obs) usaremos a funcao View() para ver a base completa e entende-la para encontrar a observacao repetida.\nEncontramos a observacao 193, com a data 12-2015 (1) e valor de 418.13. Como nao pesquisei a fundo o motivo, vou considerar como um reajuste no valor, entao irei retirar a observacao anterior (192).\nVou utilizar novamente o codigo acima, por√©m com um upgrade, adicionarei a funcao slice() para recortar a observacao 192.\nCB &lt;- CB %&gt;%                   # substituindo o dataframe\n  slice(-192) %&gt;%              # funcao p/ recortar \n  mutate(                      # funcao p/ modificar variaveis\n    Data = rep(seq(            # Substituindo a variavel Data\n                               # por uma variavel no formato de dadtas\n                               # criando uma sequencia de datas\n      from = as.Date(\"2000-01-01\"), \n      to = as.Date(\"2021-01-01\"), \n      by =\"1 month\"\n    )),\n    Valor = as.numeric(Valor)  # modificando a variavel ¬¥Valor¬¥\n                               # para o formato numerico\n  )\nAgora vamos para a parte mais delicada de todo o desafio: criar a visualiza√ß√£o, ou seja, reproduzir um gr√°fico parecido com o gr√°fico que foi publicado pelo Economista Visual.\nAntes de construir o gr√°fico, vamos elaborar a customiza√ß√£o de uma tema para aplicar juntamente com o pacote ggplot2. Esta customiza√ß√£o nos ajudar√° a gerar um gr√°fico parecido.\nNa constru√ß√£o do tema, precisamos usar a mesma fonte que o Economista Visual usa em seu gr√°fico, mas preferir n√£o investigar qual fonte √© usada, e usei uma fonte qualquer. Decidir usar a fonte Teko, que pode ser encontrada no Google Fonts\ncustom_theme &lt;- function(){\n  font &lt;- \"Teko\"             # definindo a fonte a ser utilizada\n  theme(\n    # Elementos de grade e painel\n    panel.border =  element_blank(),       # sem borda\n    panel.grid.major.x = element_blank(),  # sem grades em X\n    panel.grid.minor.x = element_blank(),  # sem grades em X\n    panel.grid.major.y = element_line(\n    color = \"#d2d2d2\"                      # cor para a linha\n    ),                                     # da grade em Y\n    panel.grid.minor.y = element_blank(),  # sem grades menor em Y,     \n    axis.ticks = element_blank(),          # tira pontos do eixo\n    # Elementos textuais\n    plot.title = element_text(             # Titulo\n      family = font,                       # definir font\n      size = 20,                           # definir tamanho\n      face = 'bold',                       # negrito\n      color = \"black\",                     # cor da fonte\n      hjust = 0,                           # ajuste p/ esquerda*\n      vjust = 2),                          \n    \n    # *O valor de hjust e vjust sao \n    # definidos entre 0 e 1:\n    # 0 significa justificado a esquerda\n    # 1 significa justificado a direita\n    # 0.5 significa justificado ao meio\n    \n    plot.subtitle = element_text(          # Sub-titulo\n      family = font,                       # fonte\n      color=\"black\",                       # cor da fonte\n      size = 12),                          # tamanho\n      \n    plot.caption = element_text(           # Legenda\n      family = font,                       # fonte\n      size = 9,                            # tamanho da fonte\n      face = \"italic\",                     # italico\n      colour = \"#4c4c4c\",                  # cor da fonte\n      hjust = 0),                          # ajuste a esquerda\n    \n    axis.title = element_text(             # Titulo dos eixos\n      family = font,                       # fonte\n      face = 'bold',                       # negrito\n      color = \"#2e2e2e\",                   # cor da fonte\n      size = 10),                          # tamanho\n    \n    axis.text = element_text(              # Texto dos eixos\n      family = font,                       # fonte\n      color = \"#2e2e2e\",                   # cor\n      size = 9),                           # tamanho\n    \n    axis.text.x = element_text(            # margem p/ texto dos eixos\n      color = \"#2e2e2e\",                   # cor\n      margin=margin(5, b = 10)),\n    \n    axis.text.y = element_text(            # margem p/ texto dos eixos\n      color = \"#2e2e2e\",                   # cor\n      margin=margin(10, b = 20)),\n    \n    legend.position=\"bottom\",              # Posi√ß√£o da legenda\n                                           # bottom = meio-inferior\n    legend.title = element_blank(),        # Anular o t√≠tulo da legenda\n    legend.text = element_text(\n      colour=\"#2e2e2e\",                    # cor da legenda \n      family = font                        # fonte\n    ),\n    plot.background = element_rect(\n      fill = \"#f7efd8\",                    # cor de fundo do grafico\n      colour = NA\n      ),\n    panel.background = element_rect(\n      fill = \"#f7efd8\",                    # cor de fundo do painel\n      colour = NA\n      )\n    \n    # since the legend often requires manual tweaking \n    # based on plot content, don't define it here\n  )\n}\nCom esse bloco de c√≥digo acima, definimos o tema personalizado para aplicarmos ao gr√°fico. O gr√°fico do Economista Visual, usa uma fonte diferente do comum, fonte que n√£o sei qual √©, e o seu t√≠tulo e sub-t√≠tulo na cor preta com ajuste a esquerda (definimos isso no tema que customizamos acima), texto nos eixos em uma cor parecido com um cinza numa escala mais clara, a mesma cor para a legenda que tamb√©m √© ajustada a esquerda, e por fim, o gr√°fico √© do tipo linha, na cor vermelha com um efeito ‚Äúglow‚Äù, ponto s√≥lido menor que o circulo, e anota√ß√£o em texto, tudo na cor vermelha.\nO eixo X √© composto pelos anos observados (datas), e a escala do eixo X inicia no ano 2000 e termina no ano 2021 com intervalos a cada 1 ano. J√° no eixo Y, temos os valores na escala de 0 a 700, com intervalo a cada 100.\nTendo essas informa√ß√µes em mente, √© hora de elaborarmos o gr√°fico. Para isso, ser√° utilizado as fun√ß√µes do pacote ggplot2 e aplicaremos o tema customizado para ajustar a est√©tica do nosso gr√°fico ao gr√°fico do Economista Visual.\nplot &lt;- CB %&gt;%              # definindo o gr√°fico\n  ggplot(aes(x=Data)) +     # adicionando apenas o eixo X\n  geom_line(                # 1¬∞ linha do eixo Y\n    aes(y = Valor),         # definindo o Valor para o eixo Y\n    size = 3,               # espessura da linha\n    colour = '#e20000',     # cor (esolhi um verlho aleatorio)\n    alpha = 0.1             # alpha mede a transpar√™ncia\n    ) +\n  geom_line(                # 2¬∞ linha do eixo Y\n    aes(y = Valor),         # definindo o Valor para o eixo Y\n    size = 2,               # espessura (menor que a 1¬∞)\n    colour = '#e20000',     # cor (a mesma cor para todas)\n    alpha = 0.2             # transpar√™ncia (maior que a 1¬∞)\n    ) +\n  geom_line(                # 3¬∞ linha do eixo Y\n    aes(y = Valor),         # definindo o Valor para o eixo Y\n    size = 1,               # espessura (menor que a 2¬∞) \n    colour = '#e20000',     # cor\n    alpha = 0.5             # transpar√™ncia (maior que a 2¬∞)\n    ) +\n  geom_line(                # 4¬∞ linha do eixo Y\n    aes(y = Valor),         # definindo o Valor para o eixo Y\n    size = 0.75,            # espessura (menor que a 3¬∞)\n    colour = '#e20000'      # cor\n    ) +                     # sem alpha, para deixar a cor s√≥lida\n  annotate(                 # 1¬∞ anota√ß√£o 1/3\n    geom=\"text\",            # tipo de anota√ß√£o: texto\n    x=as.Date(\"2000-01-01\"), # ponto que a anota√ß√£o deve aparecer\n    y=160,                  # ajustando a altura do anota√ß√£o\n    label=\"R$ 112,22\",      # Texto de refer√™ncia\n    size=4,                 # tamanho do texto\n    color = \"#e20000\",      # cor do texto (a mesma da linha)\n    family = \"Teko\"         # fonte do texto (opcional)\n    ) +\n  annotate(                 # 1¬∞ anota√ß√£o 2/3\n    geom=\"point\",           # tipo de anota√ß√£o: ponto\n    x=as.Date(\"2000-01-01\"), # ponto que a anota√ß√£o deve aparecer\n    y=112,                  # ponto cruzado entre X e Y\n    size=3,                 # tamanho do ponto\n    color = \"#e20000\"       # cor do ponto\n    ) +\n  annotate(                 # 1¬∞ anota√ß√£o 3/3\n    geom=\"point\",           # tipo de anota√ß√£o: ponto\n    x=as.Date(\"2000-01-01\"), # ponto que a anota√ß√£o de aparecer\n    y=112,                  # ponto cruzado entre X e Y\n    size=5,                 # tamanho do ponto\n    shape=21,               # formato do ponto\n    fill=\"transparent\",     # preenchimento transparente\n    color=\"#e20000\"         # cor, aplica somente na borda \n    ) +\n  annotate(                 # 2¬∞ anota√ß√£o 1/3\n    geom=\"text\",            # tipo de anota√ß√£o: texto\n    x=as.Date(\"2016-01-01\"), # ponto que a anota√ß√£o deve aparecer\n    y=500,                  # ajustando a altura da anota√ß√£o\n    label=\"R$ 448,31\",      # texto de refer√™ncia\n    size=4,                 # tamanho do texto\n    color = \"#e20000\",      # cor do texto\n    family = \"Teko\"         # fonte do texto\n    ) +\n  annotate(                 # 2¬∞ anota√ß√£o 2/3\n    geom=\"point\",           # tipo de anota√ß√£o: ponto\n    x=as.Date(\"2016-01-01\"), # ponto que a anota√ß√£o deve aparecer\n    y=448.31,               # ponto cruzado entre X e Y\n    size=3,                 # tamanho do ponto\n    color = \"#e20000\"       # cor do ponto\n    ) +\n  annotate(                 # 2¬∞ anota√ß√£o 3/3\n    geom=\"point\",           # tipo da anota√ß√£o: ponto\n    x=as.Date(\"2016-01-01\"), # ponto que a anota√ß√£o deve aparecer\n    y=448.31,               # ponto cruzado entre X e Y\n    size=5,                 # tamanho do ponto\n    shape=21,               # formato do ponto\n    fill=\"transparent\",     # preenchimento transparente\n    color=\"#e20000\"         # cor somente na borda\n    ) +\n  annotate(                 # 3¬∞ anota√ß√£o 1/3\n    geom=\"text\",            # tipo de anota√ß√£o: texto\n    x=as.Date(\"2021-01-01\"), # ponto que a anota√ß√£o deve aparecer\n    y=700,                  # ajustando a altura da anota√ß√£o\n    label=\"R$ 654,15\",      # texto de refer√™ncia\n    size=4,                 # tamanho do texto\n    color = \"#e20000\",      # cor do texto\n    family = \"Teko\"         # fonte do texto\n    ) +\n  annotate(                 # 3¬∞ anota√ß√£o 2/3\n    geom=\"point\",           # tipo de anota√ß√£o: ponto\n    x=as.Date(\"2021-01-01\"), # ponto que a anota√ß√£o deve aparecer\n    y=654.15,               # ponto cruzado entre X e Y\n    size=3,                 # tamanho do ponto\n    color = \"#e20000\"       # cor do ponto\n    ) +\n  annotate(                 # 3¬∞ anota√ß√£o 3/3\n    geom=\"point\",           # tipo de anota√ß√£o: ponto\n    x=as.Date(\"2021-01-01\"), # ponto que a anota√ß√£o deve aparecer\n    y=654.15,               # ponto cruzado entre X e Y\n    size=5,                 # tamanho do ponto\n    shape=21,               # formato do ponto\n    fill=\"transparent\",     # preenchimento transparente\n    color=\"#e20000\"         # cor somente na borda\n    ) +\n  labs(                     # r√≥tulos\n    x = NULL,               # anula r√≥tulo do eixo X\n    y = NULL,               # anula r√≥tulo do eixo Y\n    # t√≠tulo\n    title='Valor da cesta b√°sica',\n    # sub-t√≠tulo\n    subtitle = \"Com base na cidade de S√£o Paulo-SP\",\n    # legenda\n    caption='Fonte: https://www.dieese.org.br/cesta/ \\nDataViz: @hcostax'\n    ) +\n  scale_x_date(             # definindo escalas p/ eixo X\n  date_breaks = \"1 year\",   # intervalos de 1 ano\n  date_labels = \"%Y\"        # rotular somente o ano\n  ) +\n  scale_y_continuous(       # definindo escalas p/ eixo Y\n    breaks = seq(           # definindo o intervalo\n      from = 0, \n      to = 700, \n      by = 100\n      ),\n    limits=c(0, 700),       # definindo os limites\n    labels = scales::dollar_format( # adicionando nota√ß√£o monet√°ria\n      prefix=\"R$\"           # definindo como \"R$\" real brasileiro\n      )\n  ) +\n  custom_theme()            # aplicando o tema customizado\n\n# ---\n\nprint(plot)                 # gerando o gr√°fico\n\n# ---\nEsse enorme bloco de c√≥digo gera o gr√°fico final. O resultado foi bastante satisfat√≥rio (p/ mim, rsrs). Ap√≥s executar todos esses passos temos o gr√°fico:\n\nO gr√°fico n√£o ficou exatamente igual ao gr√°fico publicado no perfil do Economista Visual no instagram, mas chegamos bem pr√≥ximo. O objetivo aqui foi reproduzir a informa√ß√£o, e para deixar mais divertido, reproduzir aos moldes do original.\nVoc√™ tamb√©m pode reproduzir isso escolhendo a cidade que quiser.\nConfira o c√≥digo completo no meu Github.\n\n\n\n\n\n\nEi! üëã, voc√™ achou meu trabalho √∫til? Considere me comprar um caf√© ‚òï, clicando aqui üëáüèª\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationFor attribution, please cite this work as:\nCosta, Henrique. 2021. ‚ÄúReproduzindo Gr√°fico Do Economista Visual\n- Valor Da Cesta B√°sica (PT-BR).‚Äù February 20, 2021. https://doi.org/10.59350/y7tgs-0v282."
  },
  {
    "objectID": "blog/risk_intro/index.html",
    "href": "blog/risk_intro/index.html",
    "title": "A few words about risk management",
    "section": "",
    "text": "I started this website some time ago with the aim of writing down the expansion of my knowledge and experience in the area of financial risk management. And also other things like programming with R, econometric modeling and a little about data science and machine learning. So I‚Äôm going to use this publication as a starting point, and start discussing texts and examples applied to financial risks.\nI am not going to give a general and complete description of what financial risk is and its management processes, as this can be easily found on sites like Wikipedia.\nBut risk, in the most basic sense, is the possibility that bad things will happen. Humans have evolved to manage risks like wild animals do. However, our awareness of risk is not always adequate for the modern world.\nBehavioral science shows that we rely too much on our instincts and personal experience, as biases distort our thought processes. Furthermore, even the way we frame risk decisions irrationally influences our willingness to take risks.\nYet surprisingly sophisticated examples of risk management can be seen early in history. In ancient times, merchants and their creditors shared the risk by tying loan repayment to the safe arrival of shipments through sea loans (i.e.¬†combining loans with a type of insurance). We can see some references here and here too.\nThe insurance contract separated from the loan contract as early as the 14th century in northern Italy, creating the first autonomous financial risk transfer instrument. From the 17th century onwards, a more methodical approach to the mathematics of risk can be traced. This was followed by the development of exchange-based risk transfer, in the form of agricultural futures contracts, in the 18th and 19th centuries. Some other references here.\nThis methodical approach continued to evolve in the 20th century and beyond, with major advances in financial theory in the 1950s; an explosion in risk management markets from the 1970s onwards; and the emergence of new instruments, such as cyber risk insurance, at the beginning of the 21st century. Risk management is an ancient art, but a young science ‚Äì and an even younger profession.\nThe way we think about risk is the biggest determinant of whether we recognize risks, assess them appropriately, measure them using appropriate risk metrics and manage them.\nIn the next publications I will bring introductory content that will analyze risk definitions, the classic risk management process, the main types of risk and the tools used to track risks and make decisions.\nMost risk management disasters are caused by the failure to recognize and/or adequately address one or more of these fundamental elements, not by the failure of some sophisticated risk management technique. Century-old financial institutions have gone bankrupt because their risk management procedures ignored a certain type of risk, misunderstood the links between risks, or did not follow the classic steps of the risk management process. As we recently observed the failure of some financial institutions, both in Brazil and around the world.\n\n\n\n\n\n\nHey! üëã, did you find my work useful? Consider buying me a coffee ‚òï, by clicking here üëáüèª\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationFor attribution, please cite this work as:\nCosta, Henrique. 2023. ‚ÄúA Few Words about Risk Management.‚Äù\nAugust 27, 2023. https://doi.org/10.59350/51paw-dza08."
  },
  {
    "objectID": "blog/modelling_foundations/index.html",
    "href": "blog/modelling_foundations/index.html",
    "title": "Trivia Series: Modelling foundations",
    "section": "",
    "text": "Modeling Principles\nAlmost all, if not all, applied science is based on the idea of a model. There are problems in the real world, and we would like to create a theory to solve such problems.\nOne of the pillars or the main foundation of quantitative modeling is its effectiveness. But how can a model be effective? By demonstrating that it is simple, reproducible, and applicable in other areas of knowledge.\nA second pillar of modeling is the ability to make decisions and choices. And such choices are, and should be, made at any and all times in the process of specifying a model.\nAnother pillar is testing. A model cannot exist without its due testing process. These tests are experiments, where choices must be made and tested to ensure the effectiveness of the model.\nA model has a simple objective: to be a simplistic representation of the real world. Based on the central limit theorem, sampling, and randomness, a model is nothing more than a specific/local/unitary idea that represents the general/total.\nThe sample will never be equal to the population, and a unit will never be the total, but a model finds a pattern that it will assume as central and how far the units are from this pattern, which we formally call ‚Äúeffect‚Äù and ‚Äúresidue‚Äù.\nThe ‚Äúeffect‚Äù is what we seek, in a simplified way, to solve the real problem, and the ‚Äúresidue‚Äù is what corroborates our effect, the residue is the acid test of the model and will demonstrate how too simple the model is, or how too complex the model is.\nIt is necessary to move between the simple and the complex to centralize the effect. If the effect is central, and the residue is coherent and parsimonious, then the model has fulfilled its main objective: in a simple way, to represent the complex reality.\nAnd here is how this process works:\n\n\nThe problems observed in the real world are very complex, and to solve them we need to look for the exact solution.\nBut if the problem is complex, the solution must be just as complex, and that is why it is necessary to simplify. We can solve some problems with approximate solutions, and this is feasible. However, an approximate solution requires an approximate problem.\nIn this sense, we observe the real problem and conditionally create a simplified version that we call an approximate problem, since it is in fact close to the real one, only some simplifications have occurred to allow us to study, estimate, test and theorize.\nFrom the approximated (simplified) problem it is possible to obtain an exact solution. Therefore, it is feasible that the exact solution of the approximated problem is the approximate solution of the real problem.\nAnd so the fundamental process of modeling occurs:\n\nThis very simplistic process allows us to see that the model is nothing more than a simplification of the real problem, as I mentioned before. Using the model, it is possible to analyze the results found without needing to have the solution. The analysis of the model serves to understand the nuances of the results in order to draw conclusions about the simplification that was created. Sometimes we will not have the solution we are looking for, but we can have conclusions, even if simplified, about the real problem.\nThe analysis and conclusions of the model allow us to generate inference, which is the deduction made based on the reasoning generated by the analysis, and from the inference we can explain, even if in a simplified way, the real problem and make estimates and predictions.\nThe final step is the decision or choice that we make based on the understanding and estimates. With this, we monitor and verify whether our model is sufficient to reach an approximate solution to the real problem. Therefore, we verify whether our choices for applying the model results are meeting the expectations based on the estimates of our model.\nIf our model is not sufficient, that is, if the estimates are not observed in the real world, we need to recalibrate this model or even specify a new one. This means that we failed in one of the steps outlined in the flows above.\nWe usually make a lot of mistakes when simplifying the problem, in the initial specification of the model. It seems trivial, and it is, but this step requires a lot of creativity, because modeling is an art. If we simplify the model too much, it loses its sensitivity to reality, and if we simplify it too little, the model can be so complex that it cannot be distinguished from the real problem, and this causes failure, due to the difficulty in obtaining an exact solution.\nThe second point where we fail the most is in decision-making and verification, because this process can be expensive. Monitoring is a cost that may never be proposed or included in a project (specifying a model). The choice of how to apply it can also be made without due proportions. In general, the decision-making process can be poorly done, generating the inconsistency that leads the model to failure.\nGenerally, the person who makes the decisions is not always the same person who collected the information, nor may it be the person who specified and executed the model, much less the person who performed the prior analysis and reached the conclusions and inferences.\nThe modeling process is a process that has an indefinite temporality, but it has steps that need to be respected, as if it were a natural rule. So far, these rules are valid and work very well. For example, we have many powerful models that are sometimes defined theoretically and, when applied, perform their role accurately.\nThere is no defined time, but it can have a cost. The longer it takes, the higher the cost. This is only valid if, and only if, time has a monetary value (such as working hours, the researcher‚Äôs salary or the project budget).\nSo, even if it is trivial, do not ignore this knowledge. A model is not specified in minutes, and executing a famous or currently fashionable algorithm is not enough. It is necessary to abstract in order to think about the complexity of the real problem, as well as to think about the conditioning paths of simplification.\nA very interesting topic in simplifying a real problem and finding an approximate solution is the idea of ceteris paribus used in economics, where the general idea is to apply, in a subjective and abstract way, an implicit equilibrium condition in which the relationship between phenomena is balanced and therefore any change in the other phenomena not used in the model has no effect, isolating only the phenomenon chosen to estimate the effect of one phenomenon on the other in order to draw conclusions, make inferences and make decisions.\nFrom now on, you already know how to start your modeling process, which is not just visiting your data lake, selecting a multitude of variables and running a convolutional neural network calculation and hoping that your model is as accurate as your accuracy test tells you it is. Sometimes, a linear regression with 3 variables is enough, and this would be following the first step of simplification.\n\n\n\n\n\n\nHey! üëã, did you find my work useful? Consider buying me a coffee ‚òï, by clicking here üëáüèª\n\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationFor attribution, please cite this work as:\nCosta, Henrique. 2024. ‚ÄúTrivia Series: Modelling\nFoundations.‚Äù August 25, 2024. https://doi.org/10.59350/rzbj1-p3865."
  },
  {
    "objectID": "blog/maths_economics/index.html",
    "href": "blog/maths_economics/index.html",
    "title": "The mathematization of Economics",
    "section": "",
    "text": "In today‚Äôs post, I want to start a discussion about the use of mathematics and statistics in economics, and with this shocking sentence:\n\n\n;document.getElementById(\"tweet-45777\").innerHTML = tweet[\"html\"];\nWhen a person studies economics for the first time, it is likely that they will not encounter ‚Äúcrazy‚Äù equations that go beyond basic mathematics. There is a lot of content to learn the various conceptual definitions such as price, supply, demand, costs, profit, etc., in addition to the various market structures.\nAs you delve deeper into the subject, you realize there is more to it than just simplistic theories and newspaper chatter. So, what would be the best way to explain the concepts of price, quantity of product sold and production cost without referring to a single example without using mathematics?\nThis paper:\n\nKATZNER, Donald W. Why mathematics in economics?. Journal of Post Keynesian Economics, v. 25, n.¬†4, p.¬†561-574, 2003.\n\nmakes a very convincing defense of the use of mathematics in economic science.\nAlthough economics is technically recognized as a social science, students who embark on a ‚Äúnebulous‚Äù adventure (undergraduate course) in this field receive (or should receive) a solid foundation in mathematics. Because determining how scarce resources can be reallocated requires a minimum understanding of mathematics, to calculate what the distribution cost will be and evaluate (quantitatively) among other measures. Thus, the field of economics is full of mathematical equations and applications.\nWhat is learned in teaching economics are mainly linear algebra, calculus and statistics. These are the basis for achieving the infamous econometrics. In algebra it is taught about total cost and total revenue. In calculus the objective is to find the derivatives of indifference and utility curves, profit maximization curves, cost minimization and growth models.\nIn statistics, economists learn about predictive models and how to determine how likely a certain event is to occur. Econometrics, for some peculiar reasons, is called ‚Äúeconomentira‚Äù for the use of models in which the final objective is to estimate and predict a certain variable.\n\n\n;document.getElementById(\"tweet-83908\").innerHTML = tweet[\"html\"];\nAs you progress through related topics, you will find examples such as market demand curves (sum of several individual demand curves) or changes in the supply and price of a commodity or calculating the price elasticity of a commodity. consumption, each concept is validated using mathematics. Definitely, a mathematical and statistical approach is needed to have clarity when we arrive at the long-awaited ‚Äúsolution‚Äù to the problems proposed to professionals in this area.\n\n\n\nA typical review for the basic econometrics final exam\n\n\nIt has been noted that in the 19th century mathematics was considered as a means of achieving truth; (rational) logic made it imperative to use mathematics to prove any theorems. Many problems posed in economics are therefore motivated to be solved by mathematics. Have these problems really been solved?\nAnalyzes and studies carried out in the field of applied economics help to explain the interdependent relationship between different variables. An example is trying to explain what causes an increase in the price of a product, such as the price of beef, or an increase in the unemployment rate, or even a drop in inflation and a reduction in the basic interest rate. Mathematical functions are used as a logical tool through models from which these phenomena of everyday life can become more understandable.\nIn fact, there is an exhaustive discussion about the importance of relevant applied work and the uses of these metrics in economic science. It is interesting to know that several economists have been awarded the Nobel Prize for applying mathematics/statistics to economics, including the first awarded in 1969 to Ragnar Frisch and JanTinbergen. The most interesting thing is that Leonid Kantorovich he won a Nobel Prize in 1975 in economics for his contribution to the theory of optimal resource utilization and he was a mathematician!\nMany students who are looking to pursue a career in economics are advised to take a Mathematics course, as applied studies are covered in mathematics. The use of mathematical models and applications in this area has been notable in the last two decades.\nEconomics ‚Äî the science that stopped being dark(In Brazilian Portuguese) , with advances in Alfred Marshall, with the well-known marginalist revolt that embraced the use of mathematics as an integral part of the economy, now more intensive than ever. Mathematics plays the main role in many sciences like physics, chemistry, etc. And it‚Äôs really the backbone of the modern economy.\nMathematics in economics is an important tool in decision making. Economists are hired by companies or governments to investigate and estimate the risk or likely outcomes of an event. Economists who work for companies in the financial market make mathematical calculations (models) to assess whether the risk of investing in a certain asset outweighs its potential benefits.\nEconomists use their mathematical ‚Äúskills‚Äù to find ways to reallocate money, even in counterintuitive ways. Using a profit-maximization chart, economists can advise a venue to sell only 75% of available tickets, rather than 100%, a strategy to maximize its profit. If the company lowers ticket prices to attract new concert-goers and fill the stadium to capacity, it could make less money than selling just 75% of the tickets at a much higher price.\nEconomists also use mathematics to determine the long-term success of a business, even when some factors are unpredictable. For example, an economist working for an airline uses forecasting based on econometric models to determine the price of fuel for two months ahead. The company uses this data to block fuel prices or to protect the fuel, the famous ‚Äúhedge‚Äù. Bijan Vasigh, author of the book ‚ÄúIntroduction to Air Transport Economics‚Äù, explains that the Southwest Airlines gained a financial advantage over other operators due to its fuel hedging strategy.\nHowever, not everything is rosy, much less a bed of roses, there are limitations to what can be done using econometric models in the economy. Economists perform mathematical calculations with imperfect information. Their economic models are useless in times of natural disasters, union strikes or any other catastrophic event. Furthermore, mathematics rarely helps economists predict irrational human behavior. A fundamental assumption of economics is that humans act rationally. However, humans often make irrational decisions based on choices and preferences, or even pure fear or love. These two factors cannot be accounted for in an economic model.\nBut there are already some techniques, including in the field of Machine Learning that try to solve such problems. But that is the subject of another post that I will make soon.\nBut the potential of these methods is recognized and therefore extremely useful, and economists are reviewing the way calculations are carried out to account for intangible effects, such as pollution. Mathematical models are necessarily based on simplifying assumptions, so they are not likely to be perfectly realistic. Mathematical models also lack the nuances that can be found in narrative models. The point is that mathematics is a tool, but it is not the only tool or even the best tool that economists should use.\nSo, what is your opinion regarding the use of mathematics and statistics in economic science? Is it possible to think about economics without at least mathematizing it?\nAdapted from original.\n\n\n\n\n\n\nHey! üëã, did you find my work useful? Consider buying me a coffee ‚òï, by clicking here üëáüèª\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationFor attribution, please cite this work as:\nCosta, Henrique. 2018. ‚ÄúThe Mathematization of Economics.‚Äù\nSeptember 21, 2018. https://doi.org/10.59350/tvp77-8gp37."
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Project Portfolio",
    "section": "",
    "text": "[PT-BR] Casting lots and placing bets: decision making under uncertainty and risk management - Talking/Presentation for the master‚Äôs program in economics (UFMT). April 16, 2024.\nFoundations of Risk Management (Working in Progress)\nCycle Theory and Risk Management (Working in Progress)\n{crmvR} - A Practical R package for IFRS 9 and CECL Credit Risk Modelling and Validation. (Working in Progress)"
  },
  {
    "objectID": "projects/index.html#quant-finance-and-risk-management",
    "href": "projects/index.html#quant-finance-and-risk-management",
    "title": "Project Portfolio",
    "section": "",
    "text": "[PT-BR] Casting lots and placing bets: decision making under uncertainty and risk management - Talking/Presentation for the master‚Äôs program in economics (UFMT). April 16, 2024.\nFoundations of Risk Management (Working in Progress)\nCycle Theory and Risk Management (Working in Progress)\n{crmvR} - A Practical R package for IFRS 9 and CECL Credit Risk Modelling and Validation. (Working in Progress)"
  },
  {
    "objectID": "projects/index.html#papers-research-working-in-progress",
    "href": "projects/index.html#papers-research-working-in-progress",
    "title": "Project Portfolio",
    "section": "Papers & Research (Working in Progress) ",
    "text": "Papers & Research (Working in Progress) \n\n[PT-BR] COSTA, Henrique & SOARES, Jadson. ‚ÄúAvalia√ß√£o espacial da inadimpl√™ncia (Non-Performing Loans) dos estados brasileiros baseado nos indicadores locais de associa√ß√µes espaciais (LISA)‚Äù. (Working in Progress)\nCOSTA, Henrique. ‚ÄúVintage Analysis: a tool for risk management‚Äù. (Working in Progress)\nCOSTA, Henrique. ‚ÄúX-ray of credit default in Brazil‚Äù. (Working in Progress)"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact ",
    "section": "",
    "text": "Contact \nYou can use this form to contact me to say hello. I‚Äôm always willing to read and listen to anyone who wants to talk about economics, data science, risk management, or statistics.\nAny time spent talking to people on the Internet about these topics is time saved. I‚Äôm the annoying friend who loves talking about curiosities and refusing to talk about normal things (I don‚Äôt know what normal people talk about).\nI also love knowing if my materials were helpful to you and how they could be improved ‚Äì especially if they could be more accessible.\nHowever, for queries related to consultancy, collaborations or speaking engagements, please visit the consultancy page.\n\n\n\n\n\n\n\n\nFull Name \nEmail Address \nMessage\n\n\nSend message"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Henrique Costa",
    "section": "",
    "text": "Hi there! I‚Äôm a quant risk management professional + Master in Applied Economics, and passionate about empirical validation analysis.\nI have ‚Äúrevealed preference‚Äù for matcha üçµ and coffee ‚òï, pizza üçï, microeconomics, econometrics, microeconometrics and R.\nA concept that I always apply is: The whole intention of empirical economics is to force theory down to Earth. - George Akerlof.\nMy motto is: Be better than average.\nAt my house, my wife and I play with our children Arya, Frodo and Lunna (Three beautiful kittens). We love and enjoy brunch on Sundays and are always willing to try great food."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Financial Cycle: Credit - Part I\n\n\n\n\n\n\nRisk\n\n\nCredit\n\n\nCycle\n\n\nDataViz\n\n\n\n\n\n\nOct 19, 2024\n\n\nHenrique Costa\n\n\n\n\n\n\n\n\n\n\n\n\nTrivia Series: Modelling foundations\n\n\n\n\n\n\nMaths\n\n\nStats\n\n\nModel\n\n\n\n\n\n\nAug 25, 2024\n\n\nHenrique Costa\n\n\n\n\n\n\n\n\n\n\n\n\nSorte e Apostas\n\n\n\n\n\n\nTomada de Decis√£o\n\n\nRiscos\n\n\n\n\n\n\nApr 16, 2024\n\n\nHenrique Costa\n\n\n\n\n\n\n\n\n\n\n\n\nTrivia Series: Regression\n\n\n\n\n\n\nR\n\n\nDataViz\n\n\n\n\n\n\nDec 20, 2023\n\n\nHenrique Costa\n\n\n\n\n\n\n\n\n\n\n\n\nA few words about risk management\n\n\n\n\n\n\nManagement\n\n\nRisk\n\n\nFinance\n\n\n\n\n\n\nAug 27, 2023\n\n\nHenrique Costa\n\n\n\n\n\n\n\n\n\n\n\n\nFor me, the choice was R or Python\n\n\n\n\n\n\nR\n\n\nPython\n\n\n\n\n\n\nJun 20, 2023\n\n\nHenrique Costa\n\n\n\n\n\n\n\n\n\n\n\n\nReproduzindo gr√°fico do Economista Visual - Valor da cesta b√°sica (PT-BR)\n\n\n\n\n\n\nR\n\n\nDataViz\n\n\n\n\n\n\nFeb 20, 2021\n\n\nHenrique Costa\n\n\n\n\n\n\n\n\n\n\n\n\nCan Economists Become Data Scientists?\n\n\n\n\n\n\nEconomics\n\n\nTheory\n\n\nData Science\n\n\nEconometrics\n\n\n\n\n\n\nOct 29, 2018\n\n\nHenrique Costa\n\n\n\n\n\n\n\n\n\n\n\n\nThe mathematization of Economics\n\n\n\n\n\n\nEconomics\n\n\nTheory\n\n\nMathematics\n\n\n\n\n\n\nSep 21, 2018\n\n\nHenrique Costa\n\n\n\n\n\n\n\n\n\n\n\n\nThe Ceteris Paribus Condition\n\n\n\n\n\n\nEconomics\n\n\nTheory\n\n\n\n\n\n\nSep 16, 2018\n\n\nHenrique Costa\n\n\n\n\n\n\nNo matching items\n\n\n  \n\nReuseCC BY 4.0CitationFor attribution, please cite this work as:\n‚ÄúBlog Posts.‚Äù n.d. https://hcostax.com/blog/."
  },
  {
    "objectID": "blog/financial_cycle_credit_01/index.html",
    "href": "blog/financial_cycle_credit_01/index.html",
    "title": "Financial Cycle: Credit - Part I",
    "section": "",
    "text": "After a really cool chat over coffee with my friend Catarina and some discussions on the topic of cycles and time series with my friends Emanuella and Patr√≠cia, I thought about writing an article here for this blog and resurrecting it, updating and improving an analysis I did some time ago and published on LinkedIn, which can be accessed through this link: Credit Granting can be used as a proxy for the Financial Cycle, which tends to present a ‚Äúboom‚Äù before the beginning of a recession.\nBased on the study ‚ÄúThe Financial Cycle in Brazil‚Äù carried out in the 3rd Project of the 2018 Agreement between FEBRABAN (Brazilian Federation of Banks) and PUC - Rio (Pontifical Catholic University of Rio de Janeiro), where the authors describe the five main stylized characteristics of the financial cycle based on the work of Borio (2012), which are:\nIn this paper, we will focus on seeing only part of the first characteristic, in which the financial cycle is described in a more parsimonious way in terms of credit.\nIf you want me to develop part 2 onwards, analyzing the other characteristics, leave a comment here or in the LinkedIn post at this link"
  },
  {
    "objectID": "blog/financial_cycle_credit_01/index.html#visualizing-gaps",
    "href": "blog/financial_cycle_credit_01/index.html#visualizing-gaps",
    "title": "Financial Cycle: Credit - Part I",
    "section": "Visualizing gaps",
    "text": "Visualizing gaps\n\nTo conclude this first article, let‚Äôs look at the graph of the financial gap through the credit cycle estimated by the HP filter and the CEEMDAN algorithm.\nFor educational purposes, the gap is the difference between potential credit and effective credit.\n\n\n\n\n\n\n\n\n\n\n\nTell me in the comments what you think of this chart. Which cycle best captures business cycles?\nTo conclude, I tried to explore the Harding & Pagan (2002) algorithm for dating cycles. In short, I applied it to the total credit granting series, to try to find short-term cycles.\n\n\n\n\n\n\n\n\n\n\n\nIf you are unable to comment here, you can go to the updated post I made on LinkedIn about this article AT THIS LINK HERE. I would really appreciate it if you would like to contribute to the text, or if you have any corrections or adjustments to suggest to me. Thank you very much for your time.\n\n\n\n\n\n\n\nHey! üëã, did you find my work useful? Consider buying me a coffee ‚òï, by clicking here üëáüèª"
  },
  {
    "objectID": "blog/r_or_python/index.html",
    "href": "blog/r_or_python/index.html",
    "title": "For me, the choice was R or Python",
    "section": "",
    "text": "Having decided to improve my skills in the short space of time left between my professional life and personal life, I started researching the available data science options. It quickly became clear that I should choose a programming language to learn and apply the techniques and the choice fell on R or Python, for me both options are good because they are free and open source and have an excellent reputation, in addition to each one having its own own community of active users, and very active!\nI don‚Äôt remember where I saw this tip, but for me it was the central factor in my decision and something worth sharing:\nIf you are a professional who uses analysis tools in a business context, and with experience in Microsoft Excel, my tip is: select R to start your journey in data science. R is a single-threaded object-oriented functional programming language, and once you understand the main commands, it is intuitive to use. As such, it‚Äôs pretty predictable, which has been great for me as a professional. R has an excellent graphics package that complements the idea of analyzing data or data wrangling, which Python programmers often turn to ggplot2 to generate beautiful visualizations, or use the ggplot2 theme within Python (lol), even though in recent years the Python‚Äôs visualization libraries have improved a lot, yet this happens frequently.\nOn the other hand, if you already have some experience in programming, or are from the computing field, Python is a much more general purpose and more readable language for those coming from this field, and everything you can do in Python is the same as most of the things R is good at.\nBoth groups have strong communities that share their knowledge on various blogs and events and I really think they are both great choices. Today the market is more heated when it comes to Python, and professionals who know both languages do very well.\nNow that I‚Äôve been using R for a few years (since 2013), I‚Äôve had the chance to use it a few times in the workplace, and I use it whenever I can, showing the resources this tool offers, creating econometric models and estimation procedures. in modeling for financial risks to provide some business insights. I have already carried out many analysis projects for private clients and I feel honored that my efforts to learn are generating results.\nA fellow Python expert with an interest in machine learning once saw the few lines of code needed to organize the dataset, train a model, and predict results, and, frankly, he was shocked. We had an awkward R-to-Python conversation trying to understand the differences between data matrices and data frames afterwards, although for me it clarified where R was strong: the ease of preparing and building a model quickly for any type of analysis.\nBut depending on who you ask, when I first came into contact with R in 2013, R probably had a slight, if not substantial, advantage over Python in user adoption for machine learning and what is now known as data science. Since then, the use of Python has grown substantially and it would be difficult to argue against the fact that Python is the new favorite, although the race may be tighter than one might expect given the enthusiasm of Python fans supporting the new and Brilliant tool with biggest hype.\nIn recent years, Python has benefited greatly from the rapid maturation of free add-ons such as the Scikit-learn machine learning framework, the Pandas data structure library, the Matplotlib graphing library, and the Jupyter notebook interface, among several other open applications. Source libraries that make it easier than ever to do data science in Python. Of course, these libraries only brought Python to par with what R and RStudio could do long ago! However, Python is comparatively fast and memory efficient ‚Äì at least relative to R ‚Äì which may have contributed to the fact that Python is now arguably the language most frequently taught in formal programs in data science. and quickly gained adoption across business domains.\nRather than indicating the imminent death of R, because Python‚Äôs rise is steep. In fact, the use of R is also growing rapidly, and R and RStudio are becoming more popular than ever. Although students sometimes ask whether it‚Äôs worth starting with R rather than jumping straight to Python, there are still many good reasons to choose to learn machine learning with R over the alternative.\nPlease note that these justifications are quite subjective ‚Äì not just mine, but any justification on the internet will be like this ‚Äì and there is no right answer for everyone, so I hesitate to put this in writing! However, as someone who still uses R almost daily as part of my work for a large corporation, here are a few things I‚Äôve noticed:\n\nAs I mentioned above, R may be more intuitive and easier to learn for people with a background in social sciences or business (such as economics, marketing, and so on), while Python may make more sense for computer scientists and other types of engineers .\nR tends to be used more like a ‚Äúcalculator‚Äù where you type a command and something happens; In general, coding in Python tends to require more consideration of loops and other program flow commands (this distinction is disappearing over time with additional functionality in Python libraries).\nR uses relatively few types of data structures (those included are adapted for data analysis) and the frequently used spreadsheet-type data format is a built-in data type; Comparatively, Python has many specialized data structures and uses libraries like NumPy or Pandas , and for array data format, each has its own syntax.\nR and its packages can be easier to install and update than Python, in part because Python is managed by some operating systems by default, and maintaining separate dependencies and environments is a challenge (modern Python installation tools and managers of packages addressed this simultaneously, in some ways making the problem worse!).\nR is typically slower and more memory-intensive than Python for data manipulation and iteration over large data structures, but if the data fits in memory, this difference is somewhat insignificant. For real? If you don‚Äôt work with BigData, you probably don‚Äôt even feel this difference; R has improved in this area, R is making data preparation faster and easier, and for data that doesn‚Äôt fit in memory there are workarounds (R with BigData), but this is admittedly one of Python‚Äôs biggest advantages.\nR has the support and vision of the Posit team (formerly known as RStudio) driving innovation and making R easier and more enjoyable to use in a unified software environment (RStudio Desktop); In contrast, Python‚Äôs innovations are occurring on multiple fronts, offering more ‚Äúright‚Äù ways to accomplish the same thing (for better or worse).\n\nHopefully, the reasons above will give you the confidence to begin your journey. There is no shame in starting with R (as some people think), whether you stay with R long-term, use it side-by-side with other languages like Python, or major in something entirely different, the fundamental principles you learn will be transferred to any language (a clear example is: if you haven‚Äôt yet learned SQL and already understand the basics of Tidyverse, you will have an easier time studying SQL, or vice versa) or tools you choose. Although code written in R is much ‚Äúprettier‚Äù to me, I highly recommend that you use the right tool for the job, whatever it may be. You may find, as I did, that R and RStudio are your tools of choice for many real-world data science and machine learning projects - even if you occasionally take advantage of Python‚Äôs unique strengths!\nIn the end, my recommendation is: just pick one and get started. Although I will still need to learn a lot more about Python to get the unique benefits each language offers, amplified through collaboration. The good thing about this is that, using RStudio, an IDE for R, it is possible to use Python and SQL as well.\n\n\n\n\n\n\nHey! üëã, did you find my work useful? Consider buying me a coffee ‚òï, by clicking here üëáüèª\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationFor attribution, please cite this work as:\nCosta, Henrique. 2023. ‚ÄúFor Me, the Choice Was R or\nPython.‚Äù June 20, 2023. https://doi.org/10.59350/83vew-1de35."
  },
  {
    "objectID": "blog/ceteris_paribus/index.html",
    "href": "blog/ceteris_paribus/index.html",
    "title": "The Ceteris Paribus Condition",
    "section": "",
    "text": "I will present here some personal reflections on a concept that is considered very simple and perhaps common in economics, but which is labeled as the height of ignorance for making an assumption that cannot be observed in reality, but which in my view is a mistake. mistake. The concept: ceteris paribus, or better said ‚Äúthe ceteris paribus condition‚Äù.\nWell, have you ever tried to imagine the meaning of ceteris paribus? The idea is somewhat simplistic, and means ‚Äúeverything else is constant‚Äù or ‚Äúall other things remaining unchanged‚Äù. Nothing new on earth, because economists, students and enthusiasts know this condition well, and learn in the academic world that in economic analysis there is always this ceteris paribus thing.\nExamining a fact under the condition that an effect X, ceteris paribus had a cause Y, or better said, that a specific effect occurs from an isolated cause without other potential causes interfering or not have no involvement.\nIf you think about this statement in depth, this condition seems a bit absurd, as in reality a specific effect never has an isolated cause, as it is a static look at dynamic events. There are many criticisms about this assumption, but most are based on a misinterpretation of the ceteris paribus condition.\nUnderstanding the literal meaning of the term does not make you an expert user of it. It is a fact that observing or studying something and having to say that if everything else is constant, it goes beyond the idea that the results may be unrealistic. However, you need to abstract to understand. Is it possible for this condition to happen? As? And when does it occur?\nIf you want to carry out a methodologically in-depth study to understand scientific applications, please consume Chapter 2 of this masterpiece:\n\nSchlicht, E. (1985). Isolation and Aggregation in Economics. [S.l.]: Springer Verlag. ISBN 0-387-15254-7.\n\nTo illustrate, I will simulate a situation and use an example where I prepare my morning coffee, and so by assumption, I will sweeten it (as sugar is not welcome in my coffee). So I have the following ingredients: water, sugar, and 100% Arabica coffee powder (my favorite). We know that to prepare coffee, roughly speaking, you just need to mix all the ingredients. Of course, as long as certain ingredients are suitable for this, as in the case of water, which must reach a preparation temperature (considered ideal) close to 90¬∞C, but there is always that classic doubt: how much sugar would be ideal for sweetening? My coffee?\nIn econometric models there is a similar methodology: to estimate a variable Y it is necessary to mix the ‚Äúeffects‚Äù of variables X, as long as some variables X meet certain conditions, as there will be moments in which it will be necessary to apply transformations to the variables, changing from level to logarithmic, applying differences, among others. As is the process of changing the water temperature.\nTo solve this problem, a common economist will say:\n\nHmm, Depends!\n\nBut a Scientist will say:\n\nFor an ex-ante analysis, it will be necessary to test hypotheses!\n\nFirstly, we outline two hypotheses that 1 liter of coffee will be prepared, and so 1 liter of water (at a temperature of 90¬∞C), 4 spoons of coffee powder (as I like strong coffee) are used, so we can say the following :\n\nAs an alternative hypothesis, the sugar value is zero, supposedly the presence of sugar in the drink is practically zero, so the drink ‚Äúneeds to be sweetened‚Äù.\nAs a null hypothesis, the sugar value is non-zero, where at least one spoon (or cube) of sugar is supposedly added, as we are curious about the positive marginal effects, so the drink is being sweetened.\n\nWe are interested in inferring about the null hypothesis, as it is from there that we consider sweetening the drink, once the alternative hypothesis occurs, simply apply the variable sugar (for a better fit of the model ) until the drink is sweetened.\nNow we can identify the marginal effects that the increase in sugar can have on the drink, and a very curious irony is that we are here looking for an optimization solution. Testing the sugar variation in order to seek an optimal point of balance, that is, where the addition of sugar can lead to a point of balance between sucrose and the acidity of the coffee, making the drink sweetened to the consumer‚Äôs taste.\nTheoretically, you add a spoonful of sugar and try it; If it‚Äôs not at the desired point, you can add a little more and try again; and so on until you reach the flavor you like most. In other words, after mixing the ingredients, only the amount of sugar will be adjusted, keeping the proportions or quantities of the other ingredients fixed. It is in this ‚Äúkeeping fixed‚Äù that the Ceteris Paribus condition occurs, that is, ‚Äúkeeping all other things unchanged‚Äù.\nThis is exactly what the ceteris paribus condition means: keeping some variables fixed in their proportions and quantities or constants (in this case, the water and coffee powder after mixing and infusing) and changing only the sugar, to understand the effect that sugar has on the variable Y (the coffee itself).\nOne of the first appearances of the use of this term in economics is contained in the work of the 17th century economist Richard Cantillon, with the title:\n\nCantillon, Richard. Essays on the Nature of Commerce in General. Routledge, 2017.\n\nIn economic science, this concept is widely used in models, mainly in neoclassical theory, as it allows the interpretation of cause and effect events in the real world in a representative way.\nImagine having to deal with the enormous and growing flow of information that is generated every day, at all times? Everyone would go crazy!!! So, economic theory simplifies the analysis so that it is possible to find solutions to problems: it is like disregarding some information (or considering that they are constant, that they do not change or do not alter your analysis) and then analyze the effects of other information.\nDo not think that this is absurd, or that it is unrealistic, if for any moment this thought crossed your mind, please review it immediately, as economic theory is made up of many, many observations, deductions and tests of the events of cause and effect of the real world, and that is why it is recognized as science today.\nAnother very interesting example in which I was able to observe the effect of this condition is in the financial market, the theory of portfolio diversification guarantees the reduction of the investor‚Äôs exposure to non-systemic risk, that is, disregarding non-systemic risk and keeping it fixed close to null may be an effect of the ceteris paribus condition, as the investor diversifies his investments to protect himself from the fluctuation of such assets, trying to keep non-systemic risk to the minimum possible, and when a correlation closer to zero is achieved, he assumes that it is free of non-systemic risk and thus alters its market positions based on systemic risks. There are so many real examples that use this condition that it goes beyond sweetening my coffee.\nThe condition ceteris paribus is present in our daily lives, in almost everything it is possible to find it, but there will be skeptics who will say: ‚Äúbut in reality nothing remains fixed or constant‚Äù, or even ‚Äúthat is part of the idea of general balance, and in general it is unreal, because balance is not empirical‚Äù, well, the real world is dynamic, but in my humble opinion, the world has always been in a natural balance, the dynamism of the real world is nothing more is than equilibria interacting simultaneously, so therefore, the ceteris paribus condition can make some real and palpable sense.\nFinally, the ceteris paribus condition means that we can keep some variables fixed or constant to analyze only what matters; This condition is very useful for creating econometric models used to better calculate and interpret real-world phenomena.\nI hope I have fulfilled my mission of exploring the ceteris paribus condition in a similar way for a better understanding. In the next publications I will deal with models in economic science, to explain why this science is so full of mathematical and statistical applications.\nAdapted from original.\n\n\n\n\n\n\nHey! üëã, did you find my work useful? Consider buying me a coffee ‚òï, by clicking here üëáüèª\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationFor attribution, please cite this work as:\nCosta, Henrique. 2018. ‚ÄúThe Ceteris Paribus Condition.‚Äù\nSeptember 16, 2018. https://doi.org/10.59350/jxf43-hc691."
  },
  {
    "objectID": "blog/economists_in_data_science/index.html",
    "href": "blog/economists_in_data_science/index.html",
    "title": "Can Economists Become Data Scientists?",
    "section": "",
    "text": "When it comes to job titles, data scientist is one of the biggest buzzwords in recent years. It is also one of the fastest growing fields of professional activity. Can an economist really be a data scientist? What skills are needed?\nExposed as the ‚Äúsexiest job of the 21st century‚Äù by Harvard Business Review, here I will discuss the reasons why an economist proclaims himself as a Data Scientist, and that goes far beyond hype.\nData Scientist, this is the new field of activity that everyone is talking about these days, and that almost everyone wants to be part of it now. It is certainly a very interesting field of professional activity, as it was born from the combination of multidisciplinary and complementary techniques, which did not require much scientific formalism and academic consolidation to be called a Science or Profession. Why?\nThe term and the field of activity have existed for a long time, but thinking about the hype that perhaps it is for a simple reason: the first to give themselves the luxury of calling themselves Data Scientist, were already excellent professionals in the techniques they corroborated for its creation, that is, they were already qualified professionals in certain areas that today have been shaped as necessary techniques to be a Data Scientist.\nIt is not particularly easy to define Data Science as a whole or in main ‚Äúnecessary‚Äù techniques, but here I want to talk about why an economist can proclaim himself as a ‚ÄúData Scientist‚Äù.\nAs some people may know, on LinkedIn or elsewhere (social media), some professionals call themselves Data Scientists. But why? How did these people become data scientists? Why after studying Physics, Computer Engineering, Mathematics, Statistics, Economics or any other professional training are now suddenly part of this field? This is among other questions that many ask."
  },
  {
    "objectID": "blog/economists_in_data_science/index.html#why-can-an-economist-be-a-data-scientist",
    "href": "blog/economists_in_data_science/index.html#why-can-an-economist-be-a-data-scientist",
    "title": "Can Economists Become Data Scientists?",
    "section": "Why can an economist be a data scientist?",
    "text": "Why can an economist be a data scientist?\nThe great thing here is that it is not necessary to study this in a ‚Äúformal school‚Äù, such as professional courses, technologists or a university degree (of course, nowadays there are all these things, even postgraduate programs focused on this area , but as we are living in the information age, where a cascade of information is available on the internet and freely accessible, the need to have a certificate or diploma is almost nil), so why should I, as an economist, have the right to call me a Data Scientist?\nIf you think about it, we are used to following the path where you graduate in a specific field like Economics, so you have the right to call yourself an Economist, but what happens when a new field is developing, and you are a part of it? of this growth, and also there was no specific degree or postgraduate degree to become a part of this field?\nEconomists have a set of skills that can make them successful in this area. Economists have extensive training in articulating complex ideas, something that students in other disciplines often may not have, such as business sense where the value generated is business insight.\nWe can ask Economics students a fuzzy question or problem and they answer it with priori and posteriori analysis based on information (data), and then convert it back into understandable words that a non-economist could understand . This is a very important skill to become a Data Scientist, and one that professionals lack.\nMost Data Scientists do not approach problems like Economists do, when they carry out their studies and analyzes using econometrics. In Data Science there is no unifying theory, the objective is to predict the results of the data, the approach has its merits, and predictions prevail in the industry.\nHowever, your training as an economist will help you avoid drawing some inappropriate conclusions from the data, as many data scientists don‚Äôt have the feeling for how deep structural changes can undermine predictions.\nBut I want to suggest here that economics is ‚Äî surprisingly ‚Äî a great foundation for Data Science.\nYes! Yes! Yes! Please give me a chance to explain further. I know I‚Äôm biased, but I believe there aren‚Äôt many courses that will give you better training to work in Data Science than economics."
  },
  {
    "objectID": "blog/economists_in_data_science/index.html#ok-and-so-is-an-economist-a-data-scientist-or-not-how-it-works",
    "href": "blog/economists_in_data_science/index.html#ok-and-so-is-an-economist-a-data-scientist-or-not-how-it-works",
    "title": "Can Economists Become Data Scientists?",
    "section": "OK! And so? Is an Economist a Data Scientist or not? How it works?",
    "text": "OK! And so? Is an Economist a Data Scientist or not? How it works?\nLooking closely at the descriptions of common positions in Data Science, and the range of subjects at universities that offer undergraduate courses in economics, one can quickly deduce that economics would not be the best training to have.\nBecause most economics programs don‚Äôt teach programming languages and databases, not even about projects. What the hell is this R guy? Python? And what about Hadoop? And there‚Äôs still Hive and Pig? And now there‚Äôs TensorFlow. This has to be a joke!\nSpecific skills such as programming and databases are not included in the curriculum or the most important, however, studying Economics can provide a framework that will allow you to learn specific skills quickly. And a good economic education is indeed a solid background to have.\nThere are professionals who defend this thesis, as is the case of V√≠tor Wilher, who, in addition to having a master‚Äôs degree in economics and responsible for the website An√°lise Macro, He is also a teacher of several programming courses in the R language and data analysis.\nThis discussion about the importance of knowing how to program a language as a tool that offers an excellent relationship between analytical capacity, data collection and presentation, as well as the potential that this can offer for students and professionals, especially for young people at the beginning of their careers in Economy.\nSome reasons that Economists make great Data Scientists, and that no one tells them:\n\nEconomist already knows machine learning!\nBefore you think about stopping reading, thinking that this article is already ‚Äútravelling‚Äù or that the writer must have gone to a very strange economics college to learn about machine learning, but be careful:\nMachine learning is really just a fancy word for statistical and predictive modeling that programmers invented to make their business look better, get more attention, and even keep non-participants out of their club. Maybe they should know something about economics, after all ‚Äî scarcity raises prices! (laughter).\n\n\n;document.getElementById(\"tweet-45148\").innerHTML = tweet[\"html\"];\nA well-observed fact is that the first two modules of a machine learning course (I‚Äôm commenting on the most popular one on the Coursera website) are linear regression and logistic regression. (sarcastic laughter)\nWell, 99.99% of economists who took an introductory econometrics course, this may surprise you, but these economists probably have a deeper knowledge of linear regression than a junior or full-time data scientist.\nJust as it can be scary to come across names like ‚Äúneural networks‚Äù or ‚Äúsupport vector machines ‚Äî SVM‚Äù, the economist would possibly have to work very hard, even break a sweat to find the term ‚Äúheteroscedasticity‚Äù anywhere in machine learning programs.\nTo learn more, access these guides:\n\nHacker‚Äôs guide to Neural Networks\nA Neural Network in 11 lines of Python\n\nBut of course, neural networks can be a very deep field, much deeper than the way it has been described. Just like Recurrent nets, convolutional nets, deep learning are much more advanced and complex topics ‚Äî and their algorithms are much more powerful.\nBut for most machine learning applications, an economist should do just fine with simple models: basic neural networks, binary decision trees, regressions, SVMs. And with the statistical basis of most economics courses and econometric applications, you will have no problem understanding these concepts quickly.\n\n\nEconomists have higher standards\nCan you recite all the basic assumptions of the OLS method? What about all the possible threats to the internal and external validity of your model that could compromise your analysis?\nOf course you can, I know you are nerds. (hahaha)\nAt least in my experience as an academic, the discipline of econometrics was temporarily obsessed with finding causal relationships‚Äîand making it very clear how difficult this phenomenon is to observe without randomized controlled trials.\nNot to mention that most models are sensitive to their own basic assumptions. A serious talk would not end without someone mentioning another possible source of bias, attenuation bias, survival bias, selection bias, measurement error, reverse causality, truncation, censoring, omission, spurious correlation, etc.\nFor each problem, there was another model ‚Äî even more complicated ‚Äî to deal with it. A model that could also introduce its own baggage of assumptions and problems. The world of econometrics became confusing and more nebulous as the disciplines advanced, in addition to creating the impression of being uncertain and frustratingly limiting. Then Artificial Intelligence, Machine Learning and Data Science emerged to illuminate this dark path.\nWarning: gross exaggeration ahead.\n\n\n;document.getElementById(\"tweet-90260\").innerHTML = tweet[\"html\"];\nCompared to all this, machine learning is wonderfully, charmingly simpler. Instead of solving models explicitly ‚Äî based on strict assumptions ‚Äî they are estimated iteratively with the gradient method (and its derivatives). Rather than testing or validating the theory behind the event you are trying to study, and carefully selecting explanatory variables and the appropriate model, you can try everything you can think of and see if the answer holds up.\n\n\n\nAlbert Einstein: ‚ÄúInsanity is continuing to do the same thing over and over again and expecting different results.‚Äù Machine Learning:\n\n\nGet used to cross-validation and testing, instead of t-statistics, why not try some bootstrapping? And talking about bootstrapping, there are already some studies going on the internet criticizing the use of this technique, but while the discussion does not consolidate, we will continue to use it.\nFor economists who are enthusiastic about econometrics, this may seem pure blasphemy. But this is only because the expectation is high of finding the same ML that was expected in econometrics. Inference and causal interpretation. However, most of the time, ML is aimed at predicting and finding patterns, not causality. For some models, you can‚Äôt simply say which variables are most important in predicting outcomes.\nAnd yes! Unfortunately (I bring some truths) neural networks cannot be used to explain the causal effect of the minimum wage on unemployment. But Mr.¬†Economist (who runs models) also cannot expect a model like logit (multinomial) to be used to recognize handwriting. What I want to say here is all about the correct use of the right tools in their applications ‚Äî and I‚Äôm sure econometrics teaches you very well about this.\n\n\nEconomists really know how to write coherent reports!\nIn data science it‚Äôs not just about fancy algorithms, however, unless you‚Äôre an academic researcher who just writes theoretical papers (an isolated case, and if, only if, it‚Äôs true, you probably wouldn‚Äôt be reading this anyway). , the presentation of results and writing in a simple, concise and coherent way are present in economics.\nIf an economist works as a data scientist anywhere in the ‚Äúreal world‚Äù, and will have to present his results to non-technical audiences ‚Äî managers, marketers and writers, and clients ‚Äî he will have to be able to show why your results are important and how normal people can use and act on them.\nAs an economist, I bet that most economists wrote their fair share of articles, essays, reports, presentations and dissertations and theses in their temples ‚Äî that little work or study room, gloomy and only inhabited by beings of their own species ‚Äì - at university using MS Excel, perhaps a GRTL, the bravest E-Views, or even those outliers who venture into distant lands using * Stata*.\nDon‚Äôt underestimate this skill. It might generate some comments about how archaic this is, but the fact is that probably having this skill puts the economist well ahead of most computer scientists and mathematicians, statisticians or any other professional when it comes to generating robust analyses, presenting and explaining your work clearly ‚Äî and bringing together longer texts that have structures and logic behind them (at least that‚Äôs what should happen, now whether that actually happens I don‚Äôt know, I‚Äôll let this curiosity in the air).\n\n\nLearning programming is not difficult\nUnfortunately, to be a data scientist you will probably have to write code scripts. But not excluding the fact that economists did not need to program either. It is true that using Stata can be seen as ‚Äúprogramming‚Äù, however, it is not a ‚Äúproper‚Äù programming language, but it is a great introduction for those starting out in statistical computing. And if there is a possibility to continue to graduate school, many economics programs use other languages‚ÄîPython is very common, as are R and Matlab.\nTo the delight of some, Python has become the ‚Äúlingua franca‚Äù of data science, perhaps because it is a generalist language, in addition to being a very readable and easy-to-learn language. But I particularly like R (oops! Preference revealed, successfully detected), as it not only has a large selection of libraries, but also has a widely active community, in addition to be built precisely for this purpose.\nThe preference for the R language is because it is also powerful, but the syntax is seen as an ‚ÄúABOMINATION‚Äù by programmers of other languages. Matlab is commercial software, and although it is great (and fast) at mathematical computing, and also has an open source alternative (Octave), it is not that common. Julia is a very obscure language and still a little too young to be considered a language that would be well suited to the activities of a data scientist, but it is known so far that users here in Brazil are increasing, even some professionals at the Central Bank of Brazil already use Julia.\n\n\nSo why doesn‚Äôt anyone tell you this?\nUltimately, economists should declare themselves as great data scientists, or at least own that term, or do as little as possible to acquire it. But then why doesn‚Äôt anyone at university tell them that this is a ‚Äúreal world‚Äù career choice? On the one hand, one of the reasons is that everything is relatively new. And course structures are slow to change, and how long it takes ‚Äî favoring more traditional options in finance, academia, government.\nIn fact, the college I studied at has a professor, better known as Roney who is striving to introduce this change, and add this to the training curriculum of undergraduate students, in addition to others such as professor Adriano which has also adopted the use of these programming languages in teaching economics (to be more precise, in time series econometrics), but as said: the process is slow.\nBut don‚Äôt think that economists can‚Äôt act as data scientists in these areas mentioned (finance, academia, government), quite the contrary, it‚Äôs becoming more common every day. But I also think that there is still a bit of prejudice (or perhaps, a bit of fear) in the economic world against data science, as they defend the thesis that an economist entering into data science is beneath the main cause, as they are concerned with bigger issues.\nI‚Äôm just sorry, because it‚Äôs a shame. Because economics gives its graduates a unique blend of (statistical) and soft (human) skills that are much harder to find in Mathematics departments. Computer Science, Statistics and others.\nAnd perhaps data science roles only benefit from having careful economists (econometric enthusiasts) doing the work. This way, econometricians can make the best use of ML when it comes to testing and cross-validation and algorithmic estimation approaches.\nSo give yourself a chance and get to know this area that has immense growth potential for the future, and even Google‚Äôs Chief Economist thinks the world needs more data scientists.\nSee if this catches your attention, and don‚Äôt think that just because you don‚Äôt know what Hessians are, you can‚Äôt get into Data Science.\nI didn‚Äôt intend to make this a guide for economists on how to become data scientists. But it should possibly give you a lot of things to think about ‚Äî and expand your range of possible career options. Keep an eye on the blog, as I will always be posting about this type of subject. In future publications I will be posting small applied exercises, using R or Python to awaken the reader‚Äôs curiosity to learn.\n\n\n\n\n\n\nHey! üëã, did you find my work useful? Consider buying me a coffee ‚òï, by clicking here üëáüèª"
  },
  {
    "objectID": "consultancy/index.html",
    "href": "consultancy/index.html",
    "title": "Consultancy ",
    "section": "",
    "text": "Consultancy \nI am open, mainly, to consultancy for risk assessment (methods and models).\nIf you want to make sure you are using your data (work or research) to its full potential, please contact me! I have an M.Sc. in applied economics, I have applied methods and models in all my professional experiences, I have spent more than 500 hours teaching university students (as a TA and monograph advisor in an MBA in Data Science & Analytics) and my friends You can testify that I can talk about R effortlessly for days!\nIf you want to hire me, here is a short list of things I can do for you:\n\nResearch project\nStatistical inference\nMachine Learning\nData visualization\nScientific communication\nFinancial & Non-Financial Risk Management\n\nPlease use this form for all enquiries about consultancy, collaborations, or speaking engagements.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFull Name \nEmail Address \nMessage\n\n\nSend message"
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Henrique Costa",
    "section": "",
    "text": "M.Sc. Applied Economics\nFederal University of Mato Grosso (Brazil) 2017 ‚Äî 2019\n\nB.A. Economics\nFederal University of Mato Grosso (Brazil) 2013 ‚Äî 2017"
  },
  {
    "objectID": "cv/index.html#education",
    "href": "cv/index.html#education",
    "title": "Henrique Costa",
    "section": "",
    "text": "M.Sc. Applied Economics\nFederal University of Mato Grosso (Brazil) 2017 ‚Äî 2019\n\nB.A. Economics\nFederal University of Mato Grosso (Brazil) 2013 ‚Äî 2017"
  },
  {
    "objectID": "cv/index.html#selected-work-experience",
    "href": "cv/index.html#selected-work-experience",
    "title": "Henrique Costa",
    "section": "Selected Work Experience ",
    "text": "Selected Work Experience \n\nSenior Credit and Market Risk Analyst\nCredsystem, S√£o Paulo, SP - Brazil Jun 2024 ‚Äî Present\nProviding:\n\nIntegrated Risks\nPolicies and Model Validation\nRegulatory Risk - Prudential\nStress Test Program\nCredit Risk modeling (Expected Losses models)\nIFRS (9 & 17) modeling\nCMN/BCB 4.966-21 & BCB 352-23\nMarket Risk\n\n\\(\\Delta\\) NII and \\(\\Delta\\) EVE\nRisk monitoring of market exposures (Value at Risk, VaR Stressed, CVaR-ES);\n\nEnterprise Risk\n\n\n\nStrategy, Credit Risk and Profitability\nQuintoAndar, S√£o Paulo, SP - Brazil Oct 2022 ‚Äî Jun 2024\nProviding:\n\nProfitability modeling (NPV)\nCredit Risk modeling (Expected Losses models)\nIFRS (9 & 17) modeling\nEnterprise Risk\n\n\n\nAcademic Advisor\nMBA in Data Science & Analytics - USP/Esalq, S√£o Paulo, SP - Brazil Sept 2021 ‚Äî April 2023\nMonograph advisor:\n\nCourse Conclusion Paper Guidance for MBA students in Data Science & Analytics.\n\n\n\nFinancial and Regulatory Risk\nCrefisa Bank, S√£o Paulo, SP - Brazil Feb 2022 ‚Äî Oct 2022\nProviding:\n\nRisk and Capital Management (BACEN based on the Basel agreement) for S4\nRisk monitoring of market exposures (Value at Risk, VaR Stressed, CVaR-ES)\nFinancial Risk Management regulatory reports:\n\nDaily Risk Statement - DDR,\nMarket Risk Statement - DRM,\nLiquidity Risk Statement - DRL,\nStatement of Operational Limits - DLO and\nStatement of Individual Limits - DLI.\n\n\n\n\nRisk Analyst in Structured Finance\nLiberum Ratings, S√£o Paulo, SP - Brazil Jan 2021 ‚Äî Feb 2022\nProviding:\n\nRisk assessment in Structured Finance (Specialized in Credit Rights Investment Funds - FIDC)\nAssignment and monitoring of risk classifications (ratings)\nMarket risk assessments (macroeconomic)\nAssessments of sectoral (microeconomic) risks\n\n\n\nRetirement Fund Risk Analyst\nAgenda Assessoria (Consultancy), Cuiab√°, MT - Brazil Jan 2020 ‚Äî Jan 2021\nProviding:\n\nAssessment of financial risks in investment fund portfolios\nDevelopment of investment policies for retirement funds\nMonitoring of Economic Indicators\nStress scenarios based on Foward-looking analysis\nTechnical report of investment portfolio for Own Social Security System - RPPS (Retirement Fund Risk Assessments)\nMarket risk assessments (macroeconomic)\nAssessments of sectoral (microeconomic) risks\n\n\n\nResearcher and Data Analyst (Consultancy)\nFiemt - Federation of Industries in the State of Mato Grosso, Cuiab√°, MT - Brazil ‚Äî\nProviding:\n\nData wrangling process for industrial sector data.\nBuilding dashboards (PowerBI)\nMacroeconomic reports\n\n\n\nCorporate Credit Analyst\nGerencial Factoring Fomento Mercantil, Cuiab√°, MT - Brazil Jan 2018 ‚Äî Oct 2019\nProviding:\n\nCredit and corporate risk analysis for credit assignment.\nOperations for acquiring credit rights (anticipation of receivables).\n\n\n\nAccounting and Finance Department Intern\nMato Grosso Cotton Growers Association - AMPA, Cuiab√°, MT - Brazil Dec 2015 ‚Äî Dec 2016\n\nIntern, in the Accounting and Financial Control Department of the Mato Grosso Association of Cotton Producers (AMPA - Algod√£o de MT).\n\n\n\nTeaching Assistant - Econometrics\nFederal University of Mato Grosso (Brazil) July 2014 ‚Äî Jan 2019\n\n(When I was in undergraduate) Helping undergraduate students to understand the theoretical and practical basis for solving homework assignments, in addition to application in R language\nSupport for students who participated in the Introductory Data Science courses in R Language\n(When I was in my master‚Äôs degree) Assisting the econometrics teacher in introductory classes, applying and correcting econometrics tests, in addition to providing support with homework\n\n\n\nResearch Assistant\nFederal University of Mato Grosso (Brazil) Jan 2014 ‚Äî Dec 2015\n\nParticipating student, researcher assistant, (execution, economic statistics, production and market analysis) in the research project, Santander Universidade Solid√°ria - Sustainable Development in the Pantaneira Hydrographic Basin: implementation of agroecological practices in Cooperangi - Pocon√© - MT, Banco Santander."
  }
]