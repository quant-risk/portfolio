{
  "hash": "1f2f630be771618272ec5a116773f8c7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Testing Assumptions and Estimating Parameters Using Simulations\nsubtitle: Saving your time, energy, and sanity by simulating data\nauthor: Paul Johnson\ndate: 2024-02-29\nimage: pipe.jpg\ncategories:\n  - Simulations\n  - Statistical Power\n  - Data Science\n  - R\n# bibliography: references.bib\ndraft: true\n---\n\n\nI recently had to (sort of) reverse engineer a chi-squared test in order to measure the minimum sample size for a given absolute effect size (with the typical default values for statistical power and significance level). Although packages like `{pwr}` make power calculations for common statistical tests very easy, the effect size is measured using Cohen's $\\omega$ (sometimes referred to as $\\varphi$), which is not easily interpretable in absolute terms. It's definitely possible I might have been able to reverse engineer the effect size instead, but I'm dumb as hell and that seemed like a lot of work that I don't want to do. Instead, I made some convoluted efforts to calculate $\\varphi$ from a range of sample sizes and then estimate the minimum sample size to observe an effect size this large.\n\nI think the way I went about this was ultimately correct, however, it was multiple steps and I wasn't totally confident that what I was doing wasn't incredibly stupid. My initial estimates of the minimum sample size were larger than I expected, and I immediately assumed I was wrong and I should quit my job and become a clown. Thankfully, I don't have all the gear for the clown life, so I was forced to head back into the sewage and rethink my approach. Instead, I did the thing that was obviously preferable all along but that I had avoided just because it would have required me to make more choices up-front and I wanted the \"easy\" way out -- simulation! \n\n# The Value of Simulations\n\nIgnoring for a moment the hypothesis testing debate, because I don't want to receive death threats from an old white man that looks like Gerry Adams if his weapons of choice were strongly-worded letters to the editor written in $\\LaTeX$, simulations are an incredibly powerful tool for helping us think about our data and how we should interact with it. This shouldn't be news to anyone, but I suspect there are more than a few of you out there that, like me, often attempt to take what seems like the easier route, because it feels like it requires less decisions to be made upfront.\n\nThe problem with this is that carrying out power calculations using packages like `{pwr}` doesn't really require fewer decisions. The choices are baked into the choice of the test being carried out and the assumptions that underpin that test. You're passing the buck^[Which isn't to say that `{pwr}` is bad or that the choices are wrong, it just doesn't require you to engage with those choices upfront like simulating the data does.]! Simulating your data distribution and the model that you are fitting to that data requires you to make some explicit decisions about your data generating process. This requires a little more thought, but this is a good thing, because it's a lot easier to justify your methodological approach when you have a clear understanding of everything that went into it!\n\nBesides the obvious \"knowing what you've just done is good\" justification, once you've thought through the explicit processes that you need to simulate, I think generating the simulations usually proves to be the much simpler approach anyway.\n\n# Simulating Minimum Sample Size for Chi-Squared Tests\n\nLets use a chi-squared test as an example, because there are plenty of examples out there using t-tests, and because I recently did this and it's really all about me.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulate_data <-\n  function(effect) {\n    pre <-\n      tibble(\n        intervention = rep(\"pre\", times = 1000),\n        height = sample(\n          c(\"tall\", \"small\"),\n          size = 1000, replace = TRUE,\n          prob = c(0.6, 0.4)\n        )\n      )\n\n    post <-\n      tibble(\n        intervention = rep(\"post\", times = 1000),\n        height = sample(\n          c(\"tall\", \"small\"),\n          size = 1000, replace = TRUE,\n          prob = c(0.6 + effect, 0.4 - effect)\n        )\n      )\n\n    evaluation <- rbind(pre, post)\n  }\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulate_test <-\n  function(n_trials, n_obs, effect) {\n\n    test <- data |> chisq_test(height ~ intervention)\n    \n    test$p_value\n  }\n\nestimate_power <- function(n_trials, n_obs, effect) {\n  replicate(\n    n = n_trials, \n    simulate_test(total_appointments = n_obs, effect = effect)\n    )\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# simulated_power <-\n#   simulated_effects |> \n#   mutate(\n#     p_value = \n#       furrr::future_map2(\n#         total_appointments, effect_size, \n#         ~ estimate_power(n_trials = 10000, n_obs = .x, effect = .y),\n#         .options = options\n#         )\n#     ) |> \n#   tidyr::unnest(p_value) |> \n#   summarise(\n#     power = mean(p_value < 0.05), \n#     .by = c(effect_size, total_patients, total_appointments)\n#     )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# minimum_effect <- \n#   simulated_power |> \n#   filter(power > 0.8) |> \n#   slice_min(power, by = total_patients) |> \n#   arrange(total_patients)\n```\n:::\n\n\n# Simulating the Grouping Structure in a Multilevel Regression\n\nAnother example of the ways in which simulations can be incredibly powerful is in the modelling of assumptions in a regression model.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n# Conclusion\n\nI thought about calling this blog post \"Do More Simulations\", but in the end I decided that is too broad and doesn't really tell the reader what this blog post is about. Just know that the main thing I want you to take away from this post, however, is that simulations are good and you should do more of them.\n\nI don't think anything I've said here is revolutionary, and besides providing code that can be used as a learning resource, I suspect the primary value of this blog post is in reminding people of what they already know -- the easy way out is usually worse and you should do the right thing.\n\n## Acknowledgments {.appendix}\n\nPreview image by [Igor Omilaev](https://unsplash.com/@omilaev) on [Unsplash](https://unsplash.com/photos/a-blue-pipe-on-a-yellow-and-purple-wall-HhEEKNxIbR8).\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}